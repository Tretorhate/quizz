[
  {
    "id": 1,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "According to the lecture, which of the following statements best captures the true distinction between a decentralized system and a distributed system?",
    "options": [
      "A decentralized system has no single point of failure, while a distributed system always does.",
      "A distributed system is a networked system where resources are *sufficiently* spread across multiple computers to achieve a goal, whereas a decentralized system implies they are *necessarily* spread.",
      "Decentralized systems are always peer-to-peer, while distributed systems always follow a client-server architecture.",
      "A decentralized system becomes distributed the moment you add more than one network link between its nodes."
    ],
    "answer": 1,
    "explanation": "The lecture makes this precise distinction: a decentralized system is *necessarily* spread across multiple computers, while a distributed system is *sufficiently* spread to appear as a single coherent system. The other options are either false (decentralized systems can have logical single points of failure, like the DNS root) or represent common misconceptions."
  },
  {
    "id": 2,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A team of engineers argues that their system has no single point of failure because they have fully replicated their central database server. Which of the following fallacies or misconceptions are they most directly ignoring?",
    "options": [
      "The network is reliable and the topology does not change.",
      "A single point of failure is often easier to manage and make robust, so it's not a problem.",
      "There is one administrator.",
      "A logically centralized component, like a replicated service's coordination logic, can still be a single point of failure."
    ],
    "answer": 3,
    "explanation": "The lecture warns against confusing physical and logical centralization. Even if a database is physically replicated, the system might rely on a logically centralized component to manage failover or consistency (e.g., a consensus master). If that logical component fails, the system fails. Option 1 is a classic fallacy, but option 3 is the more direct and subtle point the engineers are missing. Option 2 is a statement of fact, not a fallacy they are ignoring."
  },
  {
    "id": 3,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A developer implements a function `charge_user(user_id, amount)` that is designed to be idempotent. They choose an 'at-least-once' RPC semantic combined with client-side retries. The server crashes immediately after successfully charging the user and persisting the result, but before sending the response. The client times out and retries. What is the most accurate description of the outcome?",
    "options": [
      "The operation will be performed exactly once, as the idempotency key on the client will prevent the second request.",
      "The user will be charged twice. Idempotency only protects against duplicate processing of the same request, but the server crash meant the client's first request was never acknowledged, so the second is seen as new.",
      "The user will not be charged. The server crash caused the transaction to be rolled back.",
      "The user will be charged only once because the server-side operation is idempotent; the second request will have no effect."
    ],
    "answer": 3,
    "explanation": "This is a tricky but classic scenario. Idempotency means performing the same operation multiple times has the same effect as doing it once. If the operation is correctly implemented (e.g., it checks a persistent log of processed `(user_id, amount)` pairs before charging), the retried request will be recognized as a duplicate (or as already completed) and will not result in another charge. The key is that idempotency is a property of the server-side implementation, not the client-side key alone. The lecture emphasizes designing idempotent operations by default."
  },
  {
    "id": 4,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A system uses a message queue for processing tasks. The queue guarantees at-least-once delivery. A consumer processes a task, updates its own database, and then acknowledges (acks) the message to the queue. If the consumer crashes *after* updating its database but *before* sending the ack, what happens upon recovery?",
    "options": [
      "The task is lost because the database update was committed but the queue, not receiving an ack, will eventually re-deliver the message, leading to a duplicate operation.",
      "The system remains perfectly consistent because the queue will not re-deliver a message that has been successfully processed, assuming the consumer's connection timed out.",
      "The consumer will ignore the re-delivered message because it can check its database and see the task has already been done.",
      "The message is lost forever because the consumer crashed, and the queue will only re-deliver messages that timed out."
    ],
    "answer": 2,
    "explanation": "This illustrates the need for idempotent processing. Because the queue provides at-least-once delivery, the message will be re-delivered. If the consumer's task (e.g., updating the database) is idempotent, the consumer can safely process it again (or detect it's a duplicate and ignore it) without causing an error. Option 1 is partially true about the re-delivery, but doesn't capture the solution. Option 0 is the correct description of the resolution."
  },
  {
    "id": 5,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Given vector clock timestamps for two events, E1 = [2, 3, 1] and E2 = [2, 2, 2], what is the correct causal relationship?",
    "options": [
      "E1 happened before E2 (E1 → E2).",
      "E2 happened before E1 (E2 → E1).",
      "E1 and E2 are concurrent.",
      "The relationship cannot be determined from the vector clocks alone."
    ],
    "answer": 2,
    "explanation": "To compare vector clocks, you check if all components of one are less than or equal to the other. E1 <= E2? Check: 2 <= 2 (true), 3 <= 2 (false). So E1 is not <= E2. E2 <= E1? Check: 2 <= 2 (true), 2 <= 3 (true), 2 <= 1 (false). So E2 is not <= E1. Since neither is less than or equal to the other, the events are concurrent. This directly tests the precise concurrency detection property of vector clocks."
  },
  {
    "id": 6,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, a process receives a marker message on an incoming channel for the first time. According to the algorithm, what must the process do *immediately after* recording its own local state?",
    "options": [
      "It starts recording all subsequent messages on that channel until it receives another marker.",
      "It sends a marker message out on all its outgoing channels.",
      "It stops processing application messages until the snapshot is complete.",
      "It records the state of the incoming channel as empty."
    ],
    "answer": 1,
    "explanation": "The lecture states: \"On receiving first marker, a process: o records its local state; o sends markers on outgoing channels.\" The other options are part of the algorithm but happen at different times. Channel recording (for other channels) starts *before* receiving the marker on that specific channel. The incoming channel on which the marker was received is recorded as empty. Sending markers is the immediate next step after recording the local state."
  },
  {
    "id": 7,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "You have a distributed key-value store with N=5 replicas. You need to design a quorum configuration that can tolerate up to 2 replica failures and still complete both read and write operations. Which of the following (R, W) pairs satisfies this requirement and also provides the best read performance?",
    "options": ["R = 1, W = 5", "R = 3, W = 3", "R = 4, W = 2", "R = 5, W = 1"],
    "answer": 1,
    "explanation": "To tolerate 2 failures, you need at least 3 replicas operational (N=5, so 5-2=3). Your read and write quorums must each be <= the number of operational replicas (3) to be successful. R=3 and W=3 works, as both require 3 nodes. R=1,W=5 fails because W=5 requires all 5 nodes. R=4,W=2 fails because R=4 requires 4 operational nodes. R=5,W=1 fails because R=5 requires all 5. So (3,3) is the only viable option among these. The question also asks for best read performance, which is generally achieved with a smaller read quorum, but the constraint of tolerating 2 failures makes (3,3) the only choice, even though its read performance is not as fast as R=1."
  },
  {
    "id": 8,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "Which of the following scenarios is NOT possible under sequential consistency but IS possible under causal consistency?",
    "options": [
      "Process P1 writes a value x=1. Process P2 later reads x=1 and then writes y=2. All processes see P2's write of y=2 before P1's write of x=1.",
      "Process P3 writes a=3. Process P4 writes b=4. Some processes see P3's write before P4's, and others see P4's write before P3's.",
      "A process reads its own write immediately after issuing it, but other processes do not see that write for a long time.",
      "Process P5 reads x=5, then reads y=5, while another process sees y=5 updated before x=5."
    ],
    "answer": 1,
    "explanation": "Sequential consistency requires all processes to see the *same* order of all operations. If P3 and P4's writes are concurrent (causally unrelated), causal consistency allows different processes to see them in different orders. This is not allowed in sequential consistency, which requires a single global order. Option 0 describes a violation of causal consistency. Option 2 is possible in both models. Option 3 describes a violation of read-your-writes consistency, which both models can violate depending on their guarantees."
  },
  {
    "id": 9,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A distributed system uses uncoordinated checkpointing. Process A fails and rolls back to its last checkpoint. To continue correct operation, this rollback may force Process B to also roll back because B received a message from A after A's checkpoint. This, in turn, could force Process C to roll back, and so on. This phenomenon is known as:",
    "options": [
      "The cascading abort problem.",
      "The domino effect.",
      "A distributed deadlock.",
      "A Byzantine fault cascade."
    ],
    "answer": 1,
    "explanation": "The lecture explicitly defines the domino effect: \"In uncoordinated checkpointing, one process's rollback can force others to roll back, which forces more processes to roll back, potentially cascading to the initial state.\""
  },
  {
    "id": 10,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses a failure detector based on 'eventually perfect' semantics. What does this imply about the failure detector's behavior?",
    "options": [
      "It will always detect a crash immediately and never suspect a correct process.",
      "It may make mistakes (suspect a correct process or not detect a crash) initially, but after some (unknown) time, it will stop making mistakes.",
      "It guarantees that if a process crashes, it will eventually be suspected by every other correct process, but it may also incorrectly suspect correct processes forever.",
      "It is impossible to implement in any real-world system, even with timeouts."
    ],
    "answer": 1,
    "explanation": "The lecture defines an Eventually Perfect failure detector as one that is accurate only after some time. Option 0 describes a Perfect failure detector. Option 2 describes an Eventually Strong failure detector (strong completeness but not strong accuracy). Option 3 is false; it's impossible to have a *perfect* detector, but eventually perfect detectors are realizable with timeouts and are used in systems like Paxos and Raft."
  },
  {
    "id": 11,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "The FLP impossibility result proves that consensus is impossible in an asynchronous system with even one potential crash failure. Paxos and Raft are practical consensus algorithms that circumvent this impossibility primarily by:",
    "options": [
      "Using randomness to break symmetry, guaranteeing termination with probability 1.",
      "Assuming a stronger failure model, such as Byzantine faults, which makes the problem solvable.",
      "Relaxing the requirement for agreement (safety) in some cases to ensure liveness.",
      "Using timeouts, which effectively transforms the system model into a partially synchronous one."
    ],
    "answer": 3,
    "explanation": "The FLP result holds in a purely asynchronous model (no timing bounds). Paxos and Raft circumvent this by using timeouts and retries. This assumes that the system is *partially synchronous*: there are periods of synchrony where messages are delivered within a bounded time, and timeouts can be set large enough to eventually work. Option 1 describes a different approach (randomized algorithms) but is not the primary way Paxos/Raft work. They are deterministic. Option 2 is incorrect; Byzantine faults are harder. Option 4 is wrong because they never relax safety."
  },
  {
    "id": 12,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In a 5-node Raft cluster, a network partition occurs that isolates Node A (the current leader) and Node B from Nodes C, D, and E. Nodes C, D, and E hold an election. Node C wins and becomes the new leader for term 6. What is the state of the old leader, Node A, and how does it behave?",
    "options": [
      "Node A detects the partition and immediately steps down, becoming a follower.",
      "Node A remains the leader in its partition, continues to accept writes, but cannot commit them because it lacks a majority. It will eventually step down when it receives a heartbeat from the new leader.",
      "Node A, upon not hearing from a majority, will increment its term, start an election, and attempt to re-establish leadership.",
      "Node A continues to operate as the leader, accepting and committing writes within its partition, leading to a split-brain scenario."
    ],
    "answer": 1,
    "explanation": "In Raft, a leader maintains its leadership by receiving heartbeats from a majority. Node A, isolated with only Node B, no longer has contact with a majority. It will not receive heartbeats from a majority and will not be able to commit new log entries. It will not, however, immediately step down. It will wait for a timeout, increment its term, and start an election, but that election will fail. The key is that the new leader (C) will eventually send heartbeats that reach A (once the partition heals or via another path), and A will then step down upon seeing the higher term number. Option 3 is the process it goes through, but option 1 describes the final, stable behavior during the partition and is the most direct answer to how it behaves *as a result* of the new leader."
  },
  {
    "id": 13,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "You are designing a travel booking system that must book a flight (external, non-compensatable API), a hotel, and a car. You choose the Saga pattern over 2PC. During the execution of the Saga, the flight booking succeeds, but the hotel booking fails. What is the correct next step?",
    "options": [
      "The system should attempt to roll back the entire transaction by calling compensating transactions for the flight, hotel, and car in reverse order.",
      "The system should immediately retry the hotel booking until it succeeds, as the Saga pattern guarantees eventual success.",
      "The system should abort the entire Saga, leaving the flight booked, and rely on a manual process to clean up.",
      "The system should execute a compensating transaction for the flight only, as the car was never booked."
    ],
    "answer": 3,
    "explanation": "A Saga executes compensating transactions for steps that already succeeded. Since the flight succeeded and the hotel failed, the system must undo the flight booking by executing its compensating transaction. The car was never booked, so no compensation is needed. Option 0 describes the correct order but includes a compensating transaction for the car, which is unnecessary and might cause errors. Option 1 is wrong; Sagas don't guarantee immediate success, and retrying indefinitely is not the pattern. Option 2 describes the failure scenario, not the recovery."
  },
  {
    "id": 14,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A system is designed to be PA/EL according to the PACELC classification. During a network partition, it chooses Availability over Consistency. Under normal operation (Else), it chooses Low Latency over Consistency. Which of the following design choices is most consistent with this classification?",
    "options": [
      "Using synchronous, quorum-based replication for all writes, and routing all reads to a single leader replica to ensure strong consistency.",
      "Using asynchronous, multi-leader replication with conflict resolution based on techniques like last-write-wins or CRDTs.",
      "Using a single leader that handles all writes, which synchronously replicates to a majority of followers before acknowledging the write to the client.",
      "Using a centralized transaction coordinator to enforce strict ACID properties across all database nodes."
    ],
    "answer": 1,
    "explanation": "PA/EL systems (like Cassandra or Dynamo) prioritize availability and low latency. Asynchronous multi-leader replication allows writes to be accepted locally with low latency, and conflict resolution handles the inconsistencies that arise. This is the direct opposite of the CP/EC design choices in options 0, 2, and 3, which prioritize consistency, often at the cost of higher latency and potential unavailability during partitions."
  },
  {
    "id": 15,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "A Spark application reads a large dataset from disk, performs a `filter()` to remove many records, then a `map()` to transform the data, and finally an `action` to write the results back to disk. A junior engineer suggests adding a `cache()` after the `filter()` to improve performance. Why is this likely to be a poor optimization in this specific case?",
    "options": [
      "Caching is an action, not a transformation, and would trigger the entire job to run prematurely.",
      "The data after the `filter()` is likely much smaller than the original data, but it will be recomputed from the source every time unless cached. However, since the final operation is a write-to-disk action, caching the filtered data might not help and could waste memory because the data is only used once.",
      "Caching in Spark is always beneficial and should be done after every transformation to avoid recomputation.",
      "The `filter()` operation is narrow, so its output is already efficiently pipelined to the `map()` stage without needing to be cached."
    ],
    "answer": 1,
    "explanation": "This is a classic point about when to use caching. Caching is most beneficial when a dataset will be used in multiple actions or iterative computations. In this pipeline, the data is read, filtered, mapped, and then written out in a single action. The filtered dataset is used only once. Caching it would force Spark to materialize and store it in memory, adding overhead for no benefit. Option 0 is false; `cache()` is a transformation. Option 2 is the incorrect assumption. Option 3 is true about pipelining, but that doesn't make caching a good idea; it's the reason caching is unnecessary."
  },

  {
    "id": 16,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A system is designed with fully redundant network paths, multiple power supplies, and geographically separated data centers. However, the configuration script for a critical load balancer has a bug that, when triggered by a specific sequence of events, causes all instances to crash simultaneously. This system violates which design principle most directly?",
    "options": [
      "Distribution transparency, because the failure is not hidden from the user.",
      "Openness, because it cannot easily integrate with other systems.",
      "Scalability, because it cannot handle an increase in users.",
      "The fallacy that there is one administrator."
    ],
    "answer": 3,
    "explanation": "This is a classic example of the 'There is one administrator' fallacy. The system is physically robust but logically fragile due to a single, shared configuration error. It assumes that all components will be managed consistently and correctly, but a single flawed change from one administrator (or team) can bring the whole system down."
  },
  {
    "id": 17,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "Which of the following scenarios best illustrates the need for the 'Relocation transparency' type of distribution transparency?",
    "options": [
      "A user in Spain accesses a website whose server has been moved from Ireland to Singapore for maintenance, without the user having to change the URL they type.",
      "A database system continues to provide query results even though one of its five replicas has just crashed.",
      "A mobile app switches from a 4G connection to Wi-Fi in the middle of a file download, and the download continues without error.",
      "Two different programming languages, Java and Python, are able to invoke the same remote web service by using different data serialization formats."
    ],
    "answer": 0,
    "explanation": "Relocation transparency hides that an object (like a server) may be moved to another location while *in use*. The user's ongoing session/access is not disrupted by the move. Option 2 is more about network/migration transparency. Option 1 is about failure transparency. Option 3 is about access transparency."
  },
  {
    "id": 18,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A startup claims their new distributed database is 'infinitely scalable' because they can add more servers to handle more users. What critical aspect of scalability are they most likely ignoring?",
    "options": [
      "Size scalability, as adding servers introduces coordination overhead that can limit gains.",
      "Geographical scalability, because adding servers in the same data center doesn't solve latency for global users.",
      "Administrative scalability, because more servers mean more administrative domains and potential policy conflicts.",
      "All of the above are critical aspects they are likely ignoring."
    ],
    "answer": 3,
    "explanation": "The lecture explicitly breaks scalability into three components: size, geographical, and administrative. Claiming 'infinite scalability' typically ignores the challenges of geographical distance (latency, unreliable WAN links) and administrative boundaries (conflicting policies, security domains), in addition to the fundamental limits of size scaling."
  },
  {
    "id": 19,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A developer argues that using a message queue with 'exactly-once' semantics is the best way to ensure that a financial transaction is processed correctly without duplicates or losses. Which of the following statements best evaluates this argument?",
    "options": [
      "The developer is correct; exactly-once semantics in messaging systems guarantee that transactions are processed precisely once.",
      "The developer is partially correct, but exactly-once semantics only work if the network is reliable and messages never get reordered.",
      "The developer misunderstands the concept; 'exactly-once' in messaging typically refers to the broker's delivery guarantee to the consumer, not the idempotent processing of the transaction itself.",
      "The developer should use RPC instead, as messaging systems cannot provide any delivery guarantees."
    ],
    "answer": 2,
    "explanation": "This is a crucial distinction. 'Exactly-once delivery' from a message broker means the message will not be lost and will not be duplicated *by the broker*. However, it does not guarantee that the consumer's processing of that message (e.g., the financial transaction) will be exactly-once. The consumer still needs to be idempotent to handle potential duplicates from crashes or re-deliveries, or to use a transactional outbox pattern. The quote from Lecture 2 is relevant: 'Exactly-once is usually achievable only within a controlled scope (e.g., a single service + its database), not across arbitrary networks without assumptions.'"
  },
  {
    "id": 20,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing a system that uses asynchronous messaging. A consumer processes messages from a queue and writes results to a database. To ensure that a message is not lost, you decide to use the following pattern: read message, write to database, then acknowledge the message to the queue. What is the primary risk of this pattern?",
    "options": [
      "The message could be processed twice if the consumer crashes after writing to the database but before acknowledging.",
      "The message could be lost forever if the consumer crashes before writing to the database.",
      "The queue might become a bottleneck, as acknowledgments are synchronous.",
      "The database write might fail, causing the message to be stuck in the queue indefinitely."
    ],
    "answer": 0,
    "explanation": "This is the classic 'at-least-once' processing risk. The database write is committed, but the acknowledgment is not sent. Upon recovery, the queue (which hasn't received an ack) will re-deliver the message, leading to duplicate processing unless the database operation is idempotent. Option 1 describes a case of message loss, which this pattern actually prevents, but it's not the *primary risk* of this specific pattern; the risk is duplication, not loss."
  },
  {
    "id": 21,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A system experiences a sudden spike in traffic. The backend services become slow, and the p99 latency increases from 100ms to 5 seconds. Clients have a timeout of 2 seconds. They see the timeout, immediately retry the request, and the retry also times out. This cycle repeats, causing load on the backend to skyrocket. This phenomenon is best described as:",
    "options": [
      "A network partition.",
      "A retry storm.",
      "A deadlock.",
      "A Byzantine failure."
    ],
    "answer": 1,
    "explanation": "The lecture explicitly defines this: 'Timeout → retry → overload creates a feedback loop. The system fails harder because clients help it fail.' This is a retry storm (also known as a retry avalanche)."
  },
  {
    "id": 22,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are evolving an API that uses Protocol Buffers for serialization. You have a message definition with a required field `user_id`. In the next version, you want to deprecate this field and replace it with a more flexible `user_identifier` message that can contain either an ID, an email, or a phone number. Which approach is most compatible with maintaining both forward and backward compatibility?",
    "options": [
      "Change the type of the `user_id` field from an integer to a message type. Old clients will send integers, which the new server can automatically convert.",
      "Add a new optional `user_identifier` field. Keep the old `user_id` field but mark it as deprecated in the documentation and stop populating it on the server side. Have new clients prefer the new field.",
      "Remove the `user_id` field entirely and add the new `user_identifier` field. Clients must update immediately or they will break.",
      "Create a new version of the entire API under a different package name and have clients migrate over time."
    ],
    "answer": 1,
    "explanation": "This follows the principle of schema evolution: add optional fields, never change the type or meaning of existing fields, and never remove required fields. Option 0 is incorrect because changing the type of a field is not compatible. Option 2 breaks old clients. Option 3 is a valid but more heavy-handed approach; option 1 is the most graceful and compatible evolution path within a single schema."
  },
  {
    "id": 23,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Consider three processes with Lamport clocks. Process P1 has an event with timestamp 5. Process P2 has an event with timestamp 7. Which of the following statements is definitely TRUE?",
    "options": [
      "The event at P1 happened before the event at P2.",
      "The event at P2 happened before the event at P1.",
      "The events are concurrent.",
      "It is possible that the event at P1 happened before the event at P2, but it is not guaranteed."
    ],
    "answer": 3,
    "explanation": "Lamport clocks guarantee that if A → B, then L(A) < L(B). However, the converse is not true. L(A) < L(B) does *not* imply A → B. The event at P1 with timestamp 5 could be concurrent with the event at P2 with timestamp 7, or it could have happened before. All we know is that it's possible P1 happened before P2, but we cannot conclude it definitively."
  },
  {
    "id": 24,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, a process Q receives a marker message on channel C from process P. This is the first marker Q has received. According to the algorithm, Q must record the state of channel C as:",
    "options": [
      "The sequence of messages received on C after Q records its local state, up until the marker arrives.",
      "The sequence of messages received on C before Q records its local state.",
      "Empty.",
      "The sequence of messages that were sent on C by P but not yet received by Q at the time P recorded its snapshot."
    ],
    "answer": 2,
    "explanation": "The lecture states: 'On receiving first marker, a process: o records its local state; o sends markers on outgoing channels.' It also implies that the channel on which the marker is received is recorded as empty because the marker itself serves as a delimiter. Any messages received before the marker are part of the local state (or were already processed), and any messages after the marker will be recorded as part of that channel's state. So the state of the channel *at the moment of the snapshot* is empty because the marker is the first thing received after the snapshot was initiated on the sender's side."
  },
  {
    "id": 25,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Vector clocks are able to detect concurrent events, while Lamport clocks cannot. What is the fundamental reason for this difference in capability?",
    "options": [
      "Vector clocks use a larger integer size than Lamport clocks, allowing them to store more information.",
      "Vector clocks maintain knowledge of the logical time of all other processes, allowing a pairwise comparison of causal history.",
      "Vector clocks require synchronized physical clocks to operate, while Lamport clocks do not.",
      "Lamport clocks only track events within a single process, while vector clocks track events across all processes."
    ],
    "answer": 1,
    "explanation": "A Lamport clock is a single number, so it can only provide a partial ordering. A vector clock is an array where each component tracks the latest event known from a specific process. By comparing two vector clocks, you can see if one knows about all the events of the other (causal order) or if each has knowledge the other lacks (concurrency). Option 0 is a simplification; it's the structure, not just the size. Option 2 is false. Option 3 is false; Lamport clocks also track events across processes via messages."
  },
  {
    "id": 26,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "A distributed key-value store uses chain replication. Which of the following statements about its behavior is TRUE?",
    "options": [
      "Any node in the chain can handle write requests, which are then propagated to the tail.",
      "Read requests can be handled by any node in the chain to distribute load.",
      "The head node is responsible for acknowledging writes to the client after the write has been persisted locally.",
      "The tail node always has the most up-to-date copy of the data and is the only node that serves read requests."
    ],
    "answer": 3,
    "explanation": "The lecture on chain replication is explicit: 'Writes flow from head to tail; reads from tail only.' and 'TAIL always has all committed data, ensuring strong consistency for reads.' Option 0 is false; only the head accepts writes. Option 1 is false; only the tail serves reads. Option 2 is false; the tail acknowledges the write after it has propagated through the entire chain."
  },
  {
    "id": 27,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "You have a quorum-based replicated database with N=7 replicas. You want the system to be able to tolerate up to 2 replica failures and still complete both read and write operations. Which of the following (R, W) configurations meets this requirement while also providing the best possible write performance (i.e., the smallest write quorum)?",
    "options": ["R = 1, W = 7", "R = 3, W = 5", "R = 5, W = 3", "R = 4, W = 4"],
    "answer": 2,
    "explanation": "To tolerate 2 failures, you need at least 5 replicas operational (N=7, 7-2=5). Your write quorum (W) must be <= the number of operational replicas (5) to be successful. To have the *best* write performance, we want the smallest W possible. Let's evaluate each option's ability to survive 2 failures: Option 0 (R=1,W=7): W=7 requires all 7 nodes, fails if 2 are down. Option 1 (R=3,W=5): W=5 works if 5 nodes are up. This meets the requirement. Option 2 (R=5,W=3): W=3 works if 3 nodes are up. This also meets the requirement, and W=3 is smaller than W=5, so it has better write performance. Option 3 (R=4,W=4): W=4 works if 4 nodes are up. This also meets the requirement. Comparing the smallest W: Option 2 has W=3, which is the smallest among the viable options. Therefore, (R=5, W=3) is the answer."
  },
  {
    "id": 28,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "Under sequential consistency, which of the following executions is NOT permitted?",
    "options": [
      "Process P1: Write x=1. Process P2: Read x returns 0, then later Read x returns 1.",
      "Process P1: Write x=1. Process P2: Write x=2. Some processes see the write order as (x=1, then x=2), while others see it as (x=2, then x=1).",
      "Process P1: Write x=1. Process P1: Write x=2. Another process reads x and gets the values in the order (2, then 1).",
      "Process P1: Write x=1. Process P2: Read x returns 1. Process P3: Read x returns 0."
    ],
    "answer": 2,
    "explanation": "Sequential consistency requires that the operations of each individual process appear in the order specified by its program. Option 2 violates P1's program order: P1 wrote 1 then 2, so any other process must see those writes in that order (1 before 2), or at least not see 2 before 1. Option 0 is allowed (P2 might read stale data initially). Option 1 is allowed; sequential consistency requires a single total order, but it doesn't specify which total order (P1's write first or P2's write first). Option 3 is allowed if P2's read happened before P3's read, but P3's read could be from a stale cache; this doesn't violate the single total order."
  },
  {
    "id": 29,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A distributed system uses a failure detector that is defined as 'strongly complete' and 'eventually strongly accurate'. What does this mean in practice?",
    "options": [
      "Every crashed process is eventually suspected by every correct process, and no correct process is ever suspected.",
      "Every crashed process is eventually suspected by every correct process, and there is a time after which no correct process is suspected.",
      "Some crashed processes may never be suspected, but those that are suspected are guaranteed to have actually crashed.",
      "Every crashed process is eventually suspected by at least one correct process, and there is a time after which no correct process is suspected by any process."
    ],
    "answer": 1,
    "explanation": "Strong completeness: Every crashed process is eventually suspected by *every* correct process. Eventually strong accuracy: There is a time after which no correct process is suspected (i.e., false suspicions may occur, but they eventually stop). Option 0 describes a perfect failure detector. Option 2 describes weak completeness. Option 3 describes a combination of weak completeness and eventual strong accuracy."
  },
  {
    "id": 30,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "You are designing a checkpointing and recovery system. You choose to use coordinated checkpointing. What is the primary advantage of this approach over uncoordinated checkpointing?",
    "options": [
      "It has lower overhead during normal operation because each process checkpoints independently.",
      "It avoids the domino effect by ensuring that the set of checkpoints forms a consistent global state.",
      "It requires less stable storage because only one process needs to checkpoint at a time.",
      "It allows for more frequent checkpointing, reducing the amount of work lost on failure."
    ],
    "answer": 1,
    "explanation": "The lecture notes that the domino effect is a problem in uncoordinated checkpointing. Coordinated checkpointing solves this by having processes coordinate their checkpoints to ensure they always represent a consistent cut of the system, thus limiting rollback to the last coordinated checkpoint. Option 0 is false; coordinated checkpointing has higher overhead due to the coordination. Option 2 is false; all processes checkpoint. Option 3 is true of both methods if done frequently."
  },
  {
    "id": 31,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A group membership service is tasked with maintaining a consistent view of which processes are alive in a distributed system. A network partition occurs, splitting a 5-node cluster into two groups: {A, B} and {C, D, E}. Each group, using its failure detector, believes the other group has failed. What is the most significant risk if both groups continue to operate as independent clusters?",
    "options": [
      "A deadlock, where each group waits for the other to recover.",
      "A split-brain scenario, where both groups independently modify shared state, leading to irreconcilable inconsistencies.",
      "A cascading failure, where the failure of one node causes all nodes to crash.",
      "A performance degradation, as each group now has fewer nodes to handle the load."
    ],
    "answer": 1,
    "explanation": "This is the classic split-brain problem. The lecture mentions it in the context of view changes: 'Critical: Avoid split-brain (two groups thinking they're the system).' Both groups would continue to accept writes, and when the partition heals, merging their divergent states would be extremely difficult or impossible."
  },
  {
    "id": 32,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Two-Phase Commit (2PC) protocol, what is the state of a participant after it has voted COMMIT and received a GLOBAL-COMMIT from the coordinator, but before it has sent its final ACK?",
    "options": [
      "The participant is in the 'prepared' state, holding locks, and waiting for the coordinator's final decision.",
      "The participant has committed the transaction locally and released its locks.",
      "The participant is uncertain and must query the coordinator to determine the next step.",
      "The participant has aborted the transaction because it didn't receive a timely response from the coordinator."
    ],
    "answer": 1,
    "explanation": "Once a participant receives GLOBAL-COMMIT, it applies the decision (commits locally) and then releases locks. The ACK is just a confirmation to the coordinator. The participant is no longer blocked. Option 0 describes the state after voting COMMIT but before receiving GLOBAL-COMMIT. Option 2 is not applicable as it has the decision. Option 3 is incorrect."
  },
  {
    "id": 33,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "Consider the FLP impossibility result. Which of the following statements represents a correct understanding of its implications for practical system design?",
    "options": [
      "It proves that building a reliable consensus system is theoretically impossible, so engineers should avoid distributed systems that require agreement.",
      "It proves that no consensus algorithm can ever be correct, so systems like Paxos and Raft are fundamentally flawed.",
      "It proves that in an asynchronous system, a deterministic consensus algorithm cannot guarantee both safety and liveness in the face of a single failure. Practical systems circumvent this by using timeouts, which introduce a weak synchrony assumption.",
      "It only applies to systems with Byzantine faults; for crash-stop failures, consensus is always solvable."
    ],
    "answer": 2,
    "explanation": "This is the key takeaway from the FLP result. It highlights the need to relax the pure asynchrony assumption. Paxos and Raft use timeouts and leader election, effectively assuming the system is partially synchronous, which allows them to guarantee liveness (eventual consensus) in practice. Option 0 and 1 are overly pessimistic and misinterpret the result. Option 3 is false; FLP applies to crash-stop failures."
  },
  {
    "id": 34,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, a new leader is elected for term 7. What must this new leader do to ensure that its log is complete and authoritative before it can begin processing new client requests?",
    "options": [
      "It immediately starts accepting new client requests, assuming its log is up-to-date because it was elected by a majority.",
      "It sends a special 'heartbeat' message to all followers to establish its authority and then begins processing.",
      "It sends an 'AppendEntries' RPC with a 'prevLogIndex' and 'prevLogTerm' that is empty, forcing all followers to accept its log as the source of truth.",
      "It sends an 'AppendEntries' RPC that serves as a heartbeat and also contains the term number. Through the normal request/response flow of these RPCs, it will eventually force any conflicting entries in follower logs to match its own, a process known as log reconciliation."
    ],
    "answer": 3,
    "explanation": "The Raft protocol ensures leader completeness via the normal log replication mechanism. When followers respond to AppendEntries, they may reject the request if their log doesn't match the `prevLogIndex` and `prevLogTerm`. The leader then decrements `nextIndex` and retries, effectively forcing the follower to adopt the leader's log for any conflicting entries. This is part of the log replication and safety mechanisms, not a separate step. Option 2 is misleading; the leader doesn't have to do anything special beyond its normal operation to establish authority, but its log becomes authoritative through this process. Option 0 is incorrect; the leader cannot assume its log is complete. The leader completeness property guarantees that any committed entry is in the leader's log, but it doesn't guarantee uncommitted entries are correct."
  },
  {
    "id": 35,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "A microservices-based e-commerce platform uses the Saga pattern to handle order processing. The Saga for a successful order is: 1. Reserve Inventory, 2. Process Payment, 3. Create Order, 4. Send Confirmation Email. The compensating transaction for 'Reserve Inventory' is 'Release Inventory'. For 'Process Payment', it is 'Issue Refund'. 'Create Order' has no compensating transaction; once an order is created, it must stand. 'Send Confirmation Email' is a notification with no compensation. What is the most significant design flaw in this Saga?",
    "options": [
      "The Saga has too many steps, making it prone to failure.",
      "The 'Create Order' step is not compensatable, meaning the Saga cannot guarantee atomicity if it fails after that point.",
      "The 'Send Confirmation Email' should be the first step, not the last.",
      "There is no flaw; this is a correctly designed Saga."
    ],
    "answer": 1,
    "explanation": "For the Saga pattern to guarantee atomicity (all-or-nothing), every step that has a permanent effect must have a compensating transaction that can undo it. If the 'Create Order' step has no compensation, and the Saga fails *after* that step (e.g., the email service is down), the system is left in an inconsistent state: payment is taken, inventory is reserved, an order is created, but no confirmation is sent. This violates the all-or-nothing principle of the Saga. A better design would be to make order creation the final step, or to make it compensatable (e.g., marking the order as 'pending' and then 'confirmed' or 'cancelled')."
  },
  {
    "id": 36,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using Strict Two-Phase Locking (2PL), a transaction T1 holds a shared lock on item X. Transaction T2 requests an exclusive lock on X. What happens to T2?",
    "options": [
      "T2 is granted the exclusive lock immediately, as shared locks are compatible with exclusive locks.",
      "T2 is blocked until T1 releases its shared lock.",
      "T2 is aborted immediately to prevent a potential deadlock.",
      "T2 is granted the lock, but T1's shared lock is upgraded to an exclusive lock."
    ],
    "answer": 1,
    "explanation": "In 2PL, shared (S) and exclusive (X) locks are incompatible. If T1 holds an S lock, T2's request for an X lock must wait (block) until T1 releases its S lock during the shrinking phase. Option 0 is false; they are incompatible. Option 2 is not a standard 2PL behavior; deadlock detection is a separate mechanism. Option 3 is false; lock upgrades are possible but must be managed carefully and would still require waiting."
  },
  {
    "id": 37,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Consider the same e-commerce scenario from question 35, but now with a different design: 1. Create Order (status = 'PENDING'), 2. Process Payment, 3. Reserve Inventory, 4. Update Order (status = 'CONFIRMED'). If the 'Reserve Inventory' step fails, what should the compensating transaction(s) be?",
    "options": [
      "Execute compensating transactions for steps 2 and 1: Issue Refund and Delete Order (or mark as 'FAILED').",
      "Only execute a compensating transaction for step 2: Issue Refund. Step 1 (Create Order) is still valid.",
      "Retry the 'Reserve Inventory' step indefinitely until it succeeds.",
      "Do nothing; the system will eventually become consistent on its own."
    ],
    "answer": 0,
    "explanation": "If step 3 (Reserve Inventory) fails, the Saga must undo all steps that have already been successfully completed. In this case, step 2 (Process Payment) must be compensated (Issue Refund), and step 1 (Create Order) must be compensated. Since the order is in a 'PENDING' state, its compensation could be to mark it as 'FAILED' or delete it. This ensures the all-or-nothing property. Option 1 would leave the order in a 'PENDING' state, which is inconsistent."
  },
  {
    "id": 38,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A database system is designed to be PC/EC according to the PACELC framework. During normal operation, it chooses Consistency over Latency. Which of the following design choices is most consistent with this classification?",
    "options": [
      "Allowing reads from any replica asynchronously, with conflict resolution based on timestamps.",
      "Using a single leader that handles all writes, with synchronous replication to a majority of followers before acknowledging the write to the client.",
      "Allowing writes to be accepted by any node, with asynchronous replication and a gossip protocol to propagate updates.",
      "Using client-side caching with a time-to-live (TTL) to serve stale data quickly."
    ],
    "answer": 1,
    "explanation": "PC/EC systems prioritize consistency even at the cost of higher latency. A single leader with synchronous replication to a majority ensures strong consistency (linearizability) because a write is only acknowledged after it is durable on a majority. This incurs higher latency due to the synchronous round-trips. Options 0, 2, and 3 describe PA/EL-style designs that favor low latency and availability, often at the cost of consistency."
  },
  {
    "id": 39,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "Two events in a distributed system have vector timestamps: E1 = [3, 0, 2] and E2 = [2, 3, 1]. Which of the following statements is true?",
    "options": [
      "E1 happened before E2 (E1 → E2).",
      "E2 happened before E1 (E2 → E1).",
      "E1 and E2 are concurrent.",
      "E1 and E2 are the same event."
    ],
    "answer": 2,
    "explanation": "Compare E1 <= E2: 3 <= 2? False. Compare E2 <= E1: 2 <= 3 (true), 3 <= 0? False. Since neither vector is less than or equal to the other, the events are concurrent."
  },
  {
    "id": 40,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "Amazon's DynamoDB uses vector clocks to handle concurrent updates to the same item. In a shopping cart application, a user adds 'item A' from their laptop and 'item B' from their phone at roughly the same time. When the user later views the cart, DynamoDB returns both versions of the cart with their vector clocks. What is the expected next step in Dynamo's conflict resolution process?",
    "options": [
      "DynamoDB automatically merges the two carts based on a pre-defined last-write-wins policy using timestamps.",
      "DynamoDB returns both versions to the application (or client), which is responsible for merging them (e.g., by taking the union of items) and writing the merged result back.",
      "DynamoDB blocks one of the writes and rejects it, ensuring that only one update succeeds.",
      "DynamoDB uses a consensus algorithm to decide which version is the 'correct' one and discards the other."
    ],
    "answer": 1,
    "explanation": "The lecture on Dynamo states: 'On read, client receives all conflicting versions. Client merges conflicts (e.g., cart items: union).' Dynamo pushes the complexity of conflict resolution to the client or application layer, as different applications may have different merging semantics (union, intersection, etc.). Option 0 is a different conflict resolution strategy (LWW) that Dynamo can be configured to use, but it's not the 'expected next step' when vector clocks are used and conflicts are detected; the system returns the conflicting versions. Option 2 and 3 are not how Dynamo handles concurrent writes."
  },
  {
    "id": 41,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "A Spark application performs the following operations on an RDD: `val rdd = sc.textFile(\"hdfs://...\")` `val filtered = rdd.filter(line => line.contains(\"ERROR\"))` `val mapped = filtered.map(line => (line.split(\" \")(0), 1))` `val reduced = mapped.reduceByKey(_ + _)` `reduced.saveAsTextFile(\"hdfs://...\")`. At what point does Spark begin executing the `filter` transformation?",
    "options": [
      "Immediately when the `filter` method is called on the RDD.",
      "When the `map` method is called on the filtered RDD.",
      "When the `reduceByKey` method is called, as it is a wide transformation.",
      "When the `saveAsTextFile` action is called."
    ],
    "answer": 3,
    "explanation": "Spark uses lazy evaluation. Transformations like `filter`, `map`, and `reduceByKey` are not executed immediately. They build up a lineage DAG. The execution is triggered only when an action (like `saveAsTextFile`, `count`, `collect`) is called. At that point, Spark analyzes the DAG and schedules the stages."
  },
  {
    "id": 42,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "You are debugging a slow Spark job. In the Spark UI, you notice that one stage has a very long task execution time compared to the others, and the amount of data read by that task is significantly larger. This symptom is most indicative of:",
    "options": [
      "Network congestion during the shuffle phase.",
      "Data skew, where one partition has significantly more data than others.",
      "A failure in the cluster resource manager (YARN).",
      "Inefficient serialization, causing large object overhead."
    ],
    "answer": 1,
    "explanation": "The description is a textbook symptom of data skew. One partition is much larger, so the task processing that partition takes much longer, becoming a bottleneck for the entire stage. Option 0 might cause overall slowdowns, but not a single task with disproportionately high data volume. Option 2 would likely cause task failures, not just slowness. Option 4 would affect all tasks relatively evenly."
  },
  {
    "id": 43,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "Which of the following statements best describes the role of the Catalyst Optimizer in Spark SQL?",
    "options": [
      "It manages the execution of tasks on executors and handles fault tolerance.",
      "It is responsible for serializing data between the JVM and off-heap memory to improve performance.",
      "It applies rule-based and cost-based optimizations to the logical query plan, such as predicate pushdown and constant folding, before generating a physical plan.",
      "It provides an API for users to define custom aggregation functions."
    ],
    "answer": 2,
    "explanation": "The lecture describes Catalyst as having phases: Analysis, Logical Optimization, Physical Planning, and Code Generation. Option 0 describes the DAG Scheduler or Task Scheduler. Option 1 describes part of the Tungsten engine. Option 3 is a feature of Spark, but not the primary role of Catalyst."
  },
  {
    "id": 44,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark Structured Streaming, you have a query that reads from a Kafka topic and computes a running count of events per user, updating a state store. To achieve exactly-once semantics end-to-end, which combination of features is essential?",
    "options": [
      "Idempotent writes to the sink and at-least-once processing from Kafka.",
      "Checkpointing to a fault-tolerant, durable storage (like HDFS/S3) and a sink that supports idempotent writes or transactions.",
      "Using a micro-batch processing mode with a very small batch interval.",
      "Disabling all optimizations in the Catalyst optimizer to ensure deterministic execution."
    ],
    "answer": 1,
    "explanation": "Structured Streaming achieves exactly-once guarantees through checkpointing. The checkpoint stores the current offsets of the source (Kafka) and the state of the aggregation. In case of a failure, the query can restart from the checkpoint, reprocess the exact same range of data, and write results to the sink. However, to prevent duplicates in the sink, the sink itself must be idempotent (writing the same result twice has the same effect as once) or support transactions. Option 0 describes an at-least-once setup. Option 2 helps with latency, not exactly-once. Option 3 is unrelated."
  },
  {
    "id": 45,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A developer argues that because their system uses a distributed database with multiple replicas, it has no single point of failure and is therefore perfectly fault-tolerant. Which of the following scenarios would most directly challenge this assertion?",
    "options": [
      "A network partition isolates a minority of replicas, causing them to become unavailable for writes.",
      "A power outage affects the entire data center where all replicas are located.",
      "A bug in the database software's consensus implementation causes all replicas to enter an infinite loop and crash simultaneously.",
      "A sudden spike in read requests overwhelms the replicas, increasing latency."
    ],
    "answer": 2,
    "explanation": "This highlights the difference between physical redundancy and logical fault tolerance. The system is replicated, but it shares a common software implementation. A bug in that shared code can cause a correlated failure, taking down all replicas at once. This is a form of single point of failure that replication alone does not solve. Option 1 is a handled scenario. Option 0 describes a geographic limitation, not a logical flaw in the argument. Option 3 is a performance issue, not a failure."
  },
  {
    "id": 46,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "According to the lecture's discussion on scalability, why is 'hiding communication latencies' a technique specifically aimed at improving geographical scalability?",
    "options": [
      "Because wide-area networks (WANs) have inherently higher latency than LANs, and asynchronous communication prevents users from blocking while waiting for responses.",
      "Because synchronous communication is not possible over long distances due to the speed of light.",
      "Because hiding latencies involves compressing data, which is more effective over long distances.",
      "Because it allows you to move computation to the client, reducing the number of WAN round trips."
    ],
    "answer": 0,
    "explanation": "The lecture mentions 'Hide communication latencies' as a technique for scaling, specifically by 'making use of asynchronous communication.' Geographical scalability is hampered by high WAN latencies. Asynchronous communication allows a program to continue doing other work while waiting for a response, thus 'hiding' the latency from the user's perspective, even if the operation takes longer overall. Option 3 describes a different technique (moving computation to the client)."
  },
  {
    "id": 47,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing an RPC framework and want to provide at-most-once semantics. Which of the following mechanisms must be implemented to guarantee this semantic?",
    "options": [
      "Client-side retries with exponential backoff and a unique request ID.",
      "Server-side duplicate detection and suppression using a cache of recently processed request IDs.",
      "A combination of client-side retries and server-side idempotent operation handlers.",
      "A guarantee from the underlying transport layer (TCP) that messages will not be duplicated."
    ],
    "answer": 1,
    "explanation": "At-most-once semantics mean the operation is performed either zero or one time, but never more than once. To guarantee this, the server must be able to detect and suppress duplicates. If a request is retried (which might happen due to network issues or client timeouts), the server must recognize it as a duplicate and not execute it again. Client-side retries (Option 0) are part of an at-least-once strategy. Option 2 describes a system aiming for exactly-once, but at-most-once doesn't require retries; it may simply drop duplicate requests. Option 3 is false; TCP can present duplicates to the application in some failure scenarios (e.g., connection re-establishment)."
  },
  {
    "id": 48,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A system uses a publish-subscribe messaging pattern. A new service is added that needs to react to all 'order_created' events. The developers configure the new service to subscribe to the 'order_created' topic. However, after deployment, they notice that the new service only receives events that occurred after it subscribed. It missed all previous events. This behavior is expected if the messaging system is implemented as:",
    "options": [
      "A message queue with competing consumers.",
      "A log-based pub/sub system (like Apache Kafka) with default consumer group configuration.",
      "A traditional topic with transient subscriptions where messages are not persisted after being delivered to existing subscribers.",
      "An RPC-based system."
    ],
    "answer": 2,
    "explanation": "Traditional pub/sub systems often have transient subscriptions. If a subscriber is not active, messages published during its downtime are lost. Kafka, as a log-based system, persists messages and allows new consumers to replay history by resetting their offset. The behavior described (missing past events) is characteristic of a transient topic, not a log-based one."
  },
  {
    "id": 49,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "A distributed system uses Lamport clocks. Process A has an event with timestamp 10. Process B has an event with timestamp 10. Which of the following statements is definitely TRUE?",
    "options": [
      "The events are concurrent.",
      "The events could be concurrent, or one could have happened before the other, but they are not causally related.",
      "The events occurred at exactly the same physical time.",
      "The events are causally related."
    ],
    "answer": 1,
    "explanation": "Lamport clocks are not unique. Equal timestamps indicate that the events could be concurrent, or they could have happened before each other but had their timestamps synchronized by message exchanges. The only guarantee is that if A → B, then L(A) < L(B). With equal timestamps, we know that A does NOT happen before B, and B does NOT happen before A, because that would require strict inequality. Therefore, they must be concurrent. Option 1 is a bit redundant; the correct conclusion is that they are concurrent. Option 0 is the correct and direct answer. Option 2 is false; logical time has no relation to physical time."
  },
  {
    "id": 50,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the context of global snapshots, what is the significance of a 'consistent cut'?",
    "options": [
      "It captures the state of all processes at exactly the same moment in physical time.",
      "It ensures that if the snapshot includes the receipt of a message, it must also include the sending of that message, representing a possible system state that could have occurred.",
      "It is the only type of snapshot that can be used for deadlock detection.",
      "It minimizes the amount of data stored in the snapshot by only recording process states, not channel states."
    ],
    "answer": 1,
    "explanation": "A consistent cut respects causality: you cannot record an effect without its cause. This means any received message in the snapshot must have its corresponding send event also in the snapshot. This ensures the snapshot represents a state that the system could have been in at some point in its execution. Option 0 is impossible without a global clock. Option 2 is false; inconsistent snapshots can be useless but are not prohibited for all purposes. Option 3 is false; a consistent snapshot includes channel states."
  },
  {
    "id": 51,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "A system uses primary-backup replication with synchronous replication to backups. The primary receives a write request, applies it locally, and then sends the update to all backups. It waits for all backups to acknowledge before sending a success response to the client. One backup is located in a different data center and has a network latency of 200ms. What is the most significant downside of this configuration?",
    "options": [
      "Write latency is increased by at least 200ms, as the primary must wait for the slowest backup.",
      "Read throughput is limited because all reads must go to the primary.",
      "The system cannot tolerate the failure of the primary.",
      "Backups may become inconsistent if the primary crashes before sending updates."
    ],
    "answer": 0,
    "explanation": "Synchronous replication to a distant backup significantly impacts write latency. The primary must wait for the slowest acknowledgment. This is a classic trade-off: strong durability vs. performance. Option 1 is a general limitation of primary-backup if reads also go to primary, but the question highlights the impact of the distant backup on writes. Option 2 is false; the system can tolerate primary failure via failover. Option 3 is false; synchronous replication ensures backups are consistent before the write is acknowledged."
  },
  {
    "id": 52,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In a quorum-based system with N=7, R=4, W=4, which of the following scenarios is possible?",
    "options": [
      "A read operation returns a value that is not the most recently written value, even though no failures have occurred.",
      "A write operation is considered successful even though it was only written to 3 replicas.",
      "A read operation succeeds even though 4 replicas are unavailable.",
      "A write operation fails because it cannot obtain a quorum, even though 5 replicas are available."
    ],
    "answer": 3,
    "explanation": "For a write to succeed, it needs acknowledgments from W=4 replicas. If 5 replicas are available, it should be able to get 4 acks. So option 3 is false. Option 0 is possible in an eventually consistent system, but with R+W > N (4+4=8 > 7), and no failures, the read quorum must overlap with the write quorum, ensuring the read sees the latest write. So option 0 is not possible. Option 1 is false; W=4 requires 4 acks. Option 2 is false; if 4 replicas are unavailable, only 3 are left, so a read quorum of 4 cannot be achieved."
  },
  {
    "id": 53,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses a write-ahead log (WAL) for fault tolerance. Before updating a data page on disk, it first writes a log record describing the change. The system crashes immediately after writing the log record but before updating the data page. Upon recovery, what action is taken?",
    "options": [
      "The system does nothing; the data page is still consistent.",
      "The system uses the log record to redo the operation and update the data page.",
      "The system uses the log record to undo the operation, as it was not fully completed.",
      "The system marks the transaction as aborted and ignores the log record."
    ],
    "answer": 1,
    "explanation": "This is the 'redo' phase of recovery in a system using write-ahead logging (specifically, a steal/no-force policy). The log record is on stable storage, so the system knows the transaction intended to commit (or was at least durable). It will redo the operation to ensure the data page reflects the committed change. Option 2 describes an undo log."
  },
  {
    "id": 54,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "You are designing a failure detector for a distributed system with nodes spread across the globe, connected by the public internet. Network delays are highly variable and can sometimes be very long (up to several seconds). You need to detect failures quickly to minimize downtime, but you also want to avoid unnecessary view changes due to false positives. Which approach is most appropriate?",
    "options": [
      "Use a short, fixed timeout (e.g., 500ms) to ensure fast detection.",
      "Use a very long, fixed timeout (e.g., 30 seconds) to avoid false positives.",
      "Use an adaptive timeout mechanism that measures recent network round-trip times and adjusts the timeout dynamically, with a sliding window of acceptable delay.",
      "Do not use timeouts; instead, rely on a central coordinator to manually mark nodes as failed."
    ],
    "answer": 2,
    "explanation": "The lecture mentions that timeout selection involves a trade-off between detection speed and false positives. In a high-variability network like the public internet, a fixed timeout is either too short (causing many false positives) or too long (slow detection). An adaptive timeout, which learns from recent network conditions, is the best way to balance these requirements in such an environment."
  },
  {
    "id": 55,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Paxos consensus algorithm, what is the purpose of the proposal number?",
    "options": [
      "To uniquely identify a value being proposed.",
      "To order the proposals and ensure that if a value is chosen, all higher-numbered proposals will have the same value.",
      "To determine which node is the leader in a given round.",
      "To timestamp the proposal for debugging purposes."
    ],
    "answer": 1,
    "explanation": "Proposal numbers in Paxos establish a precedence order. The safety property of Paxos relies on the fact that once a value is chosen, any future proposal with a higher number must carry the same value. This is enforced by Phase 1, where a proposer must learn about any already-chosen values from acceptors. Option 0 is a side effect, not the primary purpose. Option 2 is related to leader election, but the proposal number itself is not a leader identifier."
  },
  {
    "id": 56,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "A 3-node Raft cluster has a leader. The leader suddenly crashes. The two followers' election timeouts are set to 150ms and 300ms, respectively. What will happen?",
    "options": [
      "Both followers will immediately start an election, leading to a split vote and a failed election.",
      "The follower with the 150ms timeout will become a candidate first, request votes, and likely become the new leader before the other follower's timeout expires.",
      "The follower with the 300ms timeout will become the leader because it waited longer, suggesting it is more stable.",
      "The cluster will remain leaderless until a majority of nodes can communicate."
    ],
    "answer": 1,
    "explanation": "Raft's randomized election timeouts are designed to make split votes unlikely. The follower with the shorter timeout will expire first. It will become a candidate, increment its term, and send RequestVote RPCs to the other follower. The other follower, still in term X, will vote for the candidate because it has not yet started its own election, granting the candidate a majority (2 out of 3 votes)."
  },
  {
    "id": 57,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In the context of distributed transactions, what is the primary difference between a nested transaction and a Saga?",
    "options": [
      "Nested transactions are for single-node databases, while Sagas are for distributed systems.",
      "Nested transactions allow subtransactions to abort and retry independently without aborting the parent, while Sagas use compensating transactions to undo already-committed work.",
      "Sagas support parallel execution of subtransactions, while nested transactions are strictly sequential.",
      "Nested transactions require two-phase commit, while Sagas do not."
    ],
    "answer": 1,
    "explanation": "The key difference is the isolation and locking model. In nested transactions, subtransactions are still part of the same overall transaction; they may hold locks, and their effects are not visible to others until the top-level commit. If a subtransaction aborts, the parent can retry it. In a Saga, each step is an independent, committed transaction. If a later step fails, already-committed steps must be undone via compensating transactions. Option 0 is false; both are used in distributed systems. Option 2 is not a defining difference. Option 3 is true, but not the primary difference."
  },
  {
    "id": 58,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Which of the following scenarios is the best candidate for using Optimistic Concurrency Control (OCC) instead of Two-Phase Locking (2PL)?",
    "options": [
      "A banking system where multiple transactions frequently update the same account balance.",
      "An inventory management system for a popular product during a flash sale, where thousands of users are trying to purchase the same limited-stock item.",
      "A data analytics platform that performs long-running, read-only queries on a mostly static dataset that is updated nightly.",
      "An airline reservation system for a popular flight where multiple agents are booking seats simultaneously."
    ],
    "answer": 2,
    "explanation": "OCC performs well in low-contention environments. Long-running read-only queries on a static dataset are unlikely to conflict, so they will rarely abort during the validation phase. They can run without acquiring locks, which is efficient. Options 0, 1, and 3 all describe high-contention scenarios where transactions would frequently abort under OCC, making 2PL or timestamp ordering a better choice."
  },
  {
    "id": 59,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A distributed system uses hybrid logical clocks (HLC). What problem does this solve that is not addressed by either physical (NTP) clocks or vector clocks alone?",
    "options": [
      "It provides a total order of all events in the system.",
      "It allows for causality tracking while also providing a timestamp that is close to physical time, useful for external systems like snapshot isolation.",
      "It eliminates the need for any clock synchronization protocol like NTP.",
      "It reduces the storage overhead of vector clocks from O(N) to O(1)."
    ],
    "answer": 1,
    "explanation": "The lecture describes HLC as combining physical time (NTP) with logical counters. It captures causality like a logical clock, but its value is always close to the physical time. This is useful for databases like CockroachDB that need to make ordering decisions based on time (e.g., for snapshot isolation) but also need to capture causality correctly across nodes. Option 0 is false; it provides a partial order. Option 2 is false; it still relies on NTP for physical time. Option 3 is false; it still has O(N) complexity in some implementations, but typically it's a fixed-size tuple."
  },
  {
    "id": 60,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In MapReduce, the 'shuffle and sort' phase is often a performance bottleneck. Which of the following techniques is LEAST effective at mitigating this bottleneck?",
    "options": [
      "Using a combiner function on the map side to reduce the amount of data transferred.",
      "Compressing the intermediate map outputs before transferring them to reducers.",
      "Increasing the number of reducers to have each process less data, even if the total data volume remains the same.",
      "Storing intermediate data on a high-latency, high-throughput network file system (NFS) instead of the local node's disk."
    ],
    "answer": 3,
    "explanation": "The shuffle phase involves transferring data from map nodes to reduce nodes. Storing intermediate data on a remote NFS would add significant network I/O and latency on top of the already network-intensive shuffle, making the bottleneck worse. MapReduce is designed to use local disk for intermediate data for performance. Options 0, 1, and 2 are standard and effective ways to mitigate the shuffle bottleneck."
  },
  {
    "id": 61,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A system is designed with a logically centralized service registry (like Netflix Eureka) that all microservices use for discovery. The registry itself is run as a cluster of three nodes for high availability. Which statement about this design is most accurate?",
    "options": [
      "It has no single point of failure because the registry is clustered.",
      "It is a decentralized system because the registry cluster is distributed.",
      "It still has a logical single point of failure: if the registry cluster becomes unavailable (e.g., due to a software bug affecting all nodes), service discovery fails, even if individual services are running.",
      "It is an example of a peer-to-peer architecture because services register themselves."
    ],
    "answer": 2,
    "explanation": "This highlights the difference between physical distribution and logical centralization. Even though the registry is physically distributed, it is a logically centralized service. A bug in the registry software could crash all three nodes (a correlated failure), making the entire discovery service unavailable and thus a single point of failure for the system. Option 0 is naive; clustering improves availability but doesn't eliminate all SPOFs, especially logical ones. Option 1 is misleading; the registry is distributed, but the system as a whole relies on it centrally. Option 4 is false."
  },
  {
    "id": 62,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "The lecture discusses the 'fallacies of distributed computing'. Which fallacy is most directly challenged by the existence of the 'slowloris' attack, where a client opens many connections to a server and sends data very slowly, tying up server resources?",
    "options": [
      "The network is secure.",
      "Latency is zero.",
      "Bandwidth is infinite.",
      "The network is reliable."
    ],
    "answer": 2,
    "explanation": "The slowloris attack exploits the assumption of 'bandwidth is infinite' and also that connections are cheap. It consumes server connection slots by keeping them open with very slow data transmission. This shows that network resources (like connection tables) are finite and can be exhausted. While option 0 is also a fallacy, the attack is more about resource exhaustion than unauthorized access."
  },
  {
    "id": 63,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "In the context of the lecture's discussion on transparency, why is 'migration transparency' considered a separate concept from 'location transparency'?",
    "options": [
      "Migration transparency only applies to mobile devices, while location transparency applies to all resources.",
      "Location transparency hides where an object is, while migration transparency hides that it moved *while in use*, implying a change that should not disrupt ongoing interaction.",
      "They are actually the same concept; the lecture uses the terms interchangeably.",
      "Migration transparency is about moving data, while location transparency is about moving processes."
    ],
    "answer": 1,
    "explanation": "The lecture distinguishes: Location transparency hides where an object is (so you can find it). Relocation (or migration) transparency hides that it may be moved to another location while in use, ensuring ongoing operations are not disrupted. A resource could be location-transparent but not migration-transparent if a move requires reconnecting."
  },
  {
    "id": 64,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A developer implements a retry mechanism with exponential backoff and jitter. What problem does the addition of 'jitter' (randomness) to the backoff interval primarily address?",
    "options": [
      "It reduces the average wait time for retries, improving performance.",
      "It prevents multiple clients that experienced a failure simultaneously from retrying in lockstep, which could create synchronized waves of load on the server (thundering herd).",
      "It ensures that retries happen more frequently, so recovery is faster.",
      "It helps the client detect permanent failures more quickly."
    ],
    "answer": 1,
    "explanation": "The lecture mentions backoff + jitter as a mitigation for retry storms. Jitter breaks synchronization among many clients retrying at the same exponential backoff intervals, preventing them from all retrying at the same time and overwhelming the recovering server. Option 0 is not the primary purpose; jitter may actually increase average wait time slightly. Option 2 is the opposite; jitter can sometimes increase wait time."
  },
  {
    "id": 65,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A system uses asynchronous messaging with a message broker. A consumer processes messages and writes results to a database. To achieve exactly-once processing semantics, which of the following patterns is necessary?",
    "options": [
      "The broker must guarantee exactly-once delivery to the consumer.",
      "The consumer must process messages idempotently and commit the message offset and the database write in a single atomic transaction (e.g., using a transactional outbox or Kafka's exactly-once semantics with idempotent producers and consumers).",
      "The consumer should acknowledge the message before processing it to avoid duplicates.",
      "The broker should use at-most-once delivery and the client should never retry."
    ],
    "answer": 1,
    "explanation": "Exactly-once processing typically requires that the effect of processing the message (database write) and the recording of the message's progress (offset commit) are atomic. If the consumer crashes after writing to the DB but before committing the offset, the message will be redelivered. Idempotent writes can mitigate duplicates, but the atomic combination is the strongest guarantee. Kafka's exactly-once semantics implement this via transactions. Option 0 is insufficient because the broker's guarantee doesn't cover the consumer's processing. Option 2 could lead to message loss if the consumer crashes after ack but before processing. Option 3 is at-most-once, which may lose messages."
  },
  {
    "id": 66,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing an API and choose Protocol Buffers (protobuf) over JSON for serialization. Which of the following is NOT a valid reason for this choice?",
    "options": [
      "Protobuf messages are typically smaller than equivalent JSON, reducing network bandwidth.",
      "Protobuf serialization/deserialization is generally faster than JSON, reducing CPU overhead.",
      "Protobuf provides a clear, enforceable schema, which helps with API evolution and validation.",
      "Protobuf is human-readable, making debugging easier without special tools."
    ],
    "answer": 3,
    "explanation": "Protobuf is a binary format and is not human-readable. This is a trade-off; JSON's human-readability is often cited as an advantage. Options 0, 1, and 2 are commonly cited advantages of protobuf over JSON."
  },
  {
    "id": 67,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Consider a distributed execution with vector clocks. Event A has VC = [2,3,1]. Event B has VC = [2,2,2]. Which of the following is true about the causal relationship?",
    "options": [
      "A happened before B.",
      "B happened before A.",
      "A and B are concurrent.",
      "The relationship cannot be determined because the vector clocks are from different process sets."
    ],
    "answer": 2,
    "explanation": "Compare A <= B: 2 <= 2 (true), 3 <= 2 (false) → not <=. Compare B <= A: 2 <= 2 (true), 2 <= 3 (true), 2 <= 1 (false) → not <=. Since neither is <= the other, they are concurrent."
  },
  {
    "id": 68,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, suppose a process P receives a marker on channel C, and this is the first marker P has received. According to the algorithm, what should P do immediately after recording its local state?",
    "options": [
      "Start recording all messages received on channel C after the marker.",
      "Send a marker message on all its outgoing channels.",
      "Stop processing application messages until the snapshot is complete.",
      "Send a marker back on channel C to acknowledge receipt."
    ],
    "answer": 1,
    "explanation": "The algorithm: upon receiving first marker, process records its local state, then sends marker on all outgoing channels. Recording of channel C's state happens before recording local state? Actually, the algorithm: when a process receives a marker on a channel, if it hasn't yet recorded its state, it records its local state first, then starts recording on all *other* channels. But the question asks: immediately after recording local state, it sends markers on all outgoing channels. Option 0 is what it does for channels other than C after recording state, but not for C. The marker on C signifies that channel's state is empty. So the immediate next step is sending markers."
  },
  {
    "id": 69,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Which of the following is an advantage of using vector clocks over Lamport clocks in a distributed system?",
    "options": [
      "Vector clocks require less storage space.",
      "Vector clocks can detect when two events are concurrent, while Lamport clocks cannot.",
      "Vector clocks always provide a total order of events, while Lamport clocks only provide a partial order.",
      "Vector clocks do not require message passing to update, while Lamport clocks do."
    ],
    "answer": 1,
    "explanation": "Vector clocks preserve causal history and allow concurrency detection. Lamport clocks only provide a partial order and cannot detect concurrency. Option 0 is false; vector clocks require O(N) space, Lamport O(1). Option 2 is false; both provide partial orders; total order can be imposed with tie-breaking but that's not causality. Option 3 is false; both require piggybacking on messages."
  },
  {
    "id": 70,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In chain replication, if the head node fails, how is the system typically reconfigured?",
    "options": [
      "A new head is elected from among the remaining nodes, and the chain order is preserved.",
      "The next node in the chain becomes the new head, and the chain shortens.",
      "All nodes hold an election to pick a new head, potentially reordering the entire chain.",
      "The system stops processing writes until the head recovers."
    ],
    "answer": 1,
    "explanation": "In chain replication, failure handling is relatively simple: if the head fails, the next node in the chain becomes the new head. If the tail fails, the previous node becomes the new tail. If a middle node fails, it is removed and the chain is stitched together. This maintains the invariant that the tail always has all committed data. Option 0 might involve an election, but the natural order simplifies recovery."
  },
  {
    "id": 71,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "You have a quorum-based system with N=5. You want to maximize write availability (i.e., be able to accept writes even if many replicas are down) while still ensuring that reads will always see the latest write. What quorum configuration should you choose?",
    "options": ["R = 1, W = 5", "R = 3, W = 3", "R = 5, W = 1", "R = 2, W = 4"],
    "answer": 3,
    "explanation": "To maximize write availability, we want W as small as possible, but we must satisfy R + W > N to ensure read-write overlap. With N=5, the smallest W that can still satisfy the inequality with some R ≤ N is W=1. But then we need R > 4, so R must be 5. That gives R=5, W=1, which has very poor read availability. So we need to balance. Let's evaluate each option for write availability (how many failures can W tolerate): Option 0 (R=1,W=5): W=5 requires all 5, so 0 failures tolerated. Option 1 (R=3,W=3): W=3 requires 3, so can tolerate 2 failures. Option 2 (R=5,W=1): W=1 requires 1, so can tolerate 4 failures (excellent write availability). Option 3 (R=2,W=4): W=4 requires 4, so can tolerate 1 failure. So for maximum write availability, option 2 is best (W=1). But the question says 'while still ensuring that reads will always see the latest write.' With R=5,W=1, reads require all 5 replicas, which is terrible for read availability, but it does satisfy R+W>N (5+1=6>5). So option 2 is correct for maximizing write availability. However, in practice, W=1 means a write might be acknowledged by only one replica and then that replica could fail before propagating, violating durability. But the question focuses on write availability, not durability. So option 2 is correct."
  },
  {
    "id": 72,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "Under causal consistency, which of the following executions is permitted?",
    "options": [
      "A user posts a comment on a photo, and then another user sees the comment before seeing the photo.",
      "Two users update their profile pictures independently. Some friends see Alice's new picture before Bob's, while others see Bob's before Alice's.",
      "A user sends a message, then immediately edits it. A recipient sees the edit before the original message.",
      "All of the above are permitted."
    ],
    "answer": 1,
    "explanation": "Causal consistency requires that causally related operations appear in order. Option 0 is a causal violation: the comment depends on the photo's existence, so they must be seen in that order. Option 2 is a causal violation: the edit depends on the original message, so they must be seen in order. Option 1 involves two independent updates with no causal relationship; they are concurrent, so different orders are allowed. So only option 1 is permitted."
  },
  {
    "id": 73,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses a 'fail-stop' failure model. Which of the following scenarios would violate the assumptions of this model?",
    "options": [
      "A process crashes and then later reboots and rejoins the system with a clean state.",
      "A process becomes unresponsive due to a deadlock, but its process is still alive.",
      "A process crashes and remains down permanently.",
      "A process experiences a network partition and cannot communicate, but is still running."
    ],
    "answer": 1,
    "explanation": "The fail-stop model assumes that when a process fails, it stops permanently and that other processes can reliably detect that it has failed. A deadlocked process is still running (alive) but unresponsive, so it is not a clean crash. It violates the 'stop permanently' part because it hasn't stopped, and detection might be ambiguous (timeout could be due to deadlock or slowness). Option 0 is fail-recovery, not fail-stop. Option 2 is exactly fail-stop. Option 3 is a network omission, not a crash."
  },
  {
    "id": 74,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "What is the main purpose of using an 'incarnation number' in a fail-recovery model when a process restarts?",
    "options": [
      "To indicate how many times the process has failed, for statistical monitoring.",
      "To distinguish the new instance of the process from the old one, so that other nodes can treat messages from the old incarnation as stale and discard them.",
      "To prioritize which process should become leader after a failure.",
      "To calculate the exponential backoff for reconnection attempts."
    ],
    "answer": 1,
    "explanation": "The lecture mentions incarnation numbers as a way to distinguish a new process instance after a crash from the previous one. This helps in failure detection and group membership: if a node receives a message from an old incarnation (with a lower incarnation number), it knows that process has restarted and can ignore stale messages. Option 0 is a side effect, not the primary purpose. Option 2 is not the main use."
  },
  {
    "id": 75,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "In the context of rollback recovery, what is the 'domino effect' and how can it be avoided?",
    "options": [
      "It is the cascading rollback of multiple processes due to uncoordinated checkpoints; it can be avoided by using coordinated checkpointing or message logging.",
      "It is the phenomenon where one failure causes all nodes to crash; it can be avoided by using redundancy.",
      "It is the effect where a slow node slows down the entire system; it can be avoided by using asynchronous communication.",
      "It is the problem of duplicate messages after recovery; it can be avoided by using idempotent operations."
    ],
    "answer": 0,
    "explanation": "The lecture defines the domino effect in uncoordinated checkpointing: one process rolls back, forcing others to roll back, potentially cascading to the initial state. Coordinated checkpointing or message logging (pessimistic/optimistic) avoids this by creating consistent recovery points."
  },
  {
    "id": 76,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Two-Phase Commit (2PC) protocol, what is the state of a participant after it has sent a 'VOTE-COMMIT' to the coordinator?",
    "options": [
      "It has committed the transaction locally and released all locks.",
      "It is in a 'prepared' state: it has made all changes durable and will not unilaterally abort, but it holds locks and waits for the coordinator's final decision.",
      "It is still uncertain and can still abort if it doesn't hear from the coordinator after a timeout.",
      "It has aborted the transaction because it voted commit but the coordinator might abort."
    ],
    "answer": 1,
    "explanation": "In 2PC, after voting COMMIT, the participant enters the prepared state. It has ensured it can commit (e.g., written to stable storage) and will not abort on its own. It holds locks and waits for the coordinator's final decision (GLOBAL-COMMIT or GLOBAL-ABORT). Option 0 is after receiving GLOBAL-COMMIT. Option 2 is incorrect; it cannot abort after voting commit. Option 3 is false."
  },
  {
    "id": 77,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "Why does Three-Phase Commit (3PC) avoid blocking, unlike 2PC?",
    "options": [
      "Because it uses a coordinator that never fails.",
      "Because it adds a 'pre-commit' phase that ensures all participants know that a majority has voted to commit, so they can safely elect a new coordinator and proceed even if the old coordinator fails.",
      "Because it requires synchronous communication, eliminating uncertainty.",
      "Because it uses a quorum-based approach instead of a coordinator."
    ],
    "answer": 1,
    "explanation": "3PC introduces a pre-commit phase. After all participants vote COMMIT, the coordinator sends PRE-COMMIT, informing everyone that a commit is imminent. If the coordinator then fails, the participants know that everyone else has voted commit, so they can safely elect a new coordinator and complete the commit. In 2PC, participants in the prepared state don't know if others have voted commit, so they cannot safely decide without the coordinator. Option 0 is false; coordinators can fail. Option 2 is false; 3PC still assumes bounded delays. Option 3 is false; it still has a coordinator."
  },
  {
    "id": 78,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, what ensures that a leader's log entries are never overwritten by another leader?",
    "options": [
      "The leader completeness property: a candidate's log must be at least as up-to-date as a majority of servers to be elected, guaranteeing that any committed entry is present in the new leader's log.",
      "The use of fsync on every write, making logs durable.",
      "The fact that followers always accept entries from the leader without question.",
      "The randomized election timeouts that prevent split votes."
    ],
    "answer": 0,
    "explanation": "Raft's election restriction ensures that a candidate cannot become leader unless its log is at least as complete as a majority of nodes. This guarantees that any committed entry (which exists on a majority) will be present in the new leader's log, and since leaders never overwrite their own logs, committed entries are safe. Option 1 helps durability but not the overwrite property. Option 2 is false; followers check log consistency. Option 3 helps avoid elections, but not the log safety."
  },
  {
    "id": 79,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "Consider the FLP impossibility result. Which of the following best describes why it does not prevent practical systems from achieving consensus?",
    "options": [
      "FLP only applies to Byzantine failures, not crash failures, and most systems assume crash failures.",
      "FLP assumes an asynchronous model with no timeouts. Practical systems use timeouts and retries, effectively operating in a partially synchronous model where the impossibility does not apply.",
      "FLP is a theoretical result that has been disproven by the existence of Paxos and Raft.",
      "FLP only applies to systems with more than two processes; most practical systems have only a few nodes."
    ],
    "answer": 1,
    "explanation": "The FLP impossibility result holds in the asynchronous model (no bounds on message delays). Practical systems like Paxos and Raft use timeouts to detect failures and progress, which introduces a weak synchrony assumption (partial synchrony), circumventing the impossibility. Option 0 is false; FLP applies to crash failures. Option 2 is false; it hasn't been disproven. Option 3 is false; it applies to any number ≥ 2."
  },
  {
    "id": 80,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "A distributed database uses strict two-phase locking (2PL) and timestamp-based deadlock detection. Transaction T1 holds a lock on item X and requests a lock on item Y. Transaction T2 holds a lock on item Y and requests a lock on item X. This is a classic deadlock. How does timestamp-based deadlock detection typically resolve this?",
    "options": [
      "It aborts the transaction with the older timestamp (wait-die) or the younger timestamp (wound-wait), depending on the policy.",
      "It randomly aborts one of the transactions.",
      "It waits indefinitely until one of the transactions releases its lock.",
      "It promotes both transactions to a higher priority and lets them proceed."
    ],
    "answer": 0,
    "explanation": "Timestamp-based deadlock prevention uses either wait-die (older waits for younger, younger aborts) or wound-wait (older wounds younger, younger aborts). This is a deterministic policy based on timestamps to break the deadlock. Option 1 is not typical; deterministic policies are used. Option 2 would lead to indefinite blocking. Option 3 is not a standard mechanism."
  },
  {
    "id": 81,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In the context of distributed transactions, what is a 'compensating transaction' in the Saga pattern?",
    "options": [
      "A transaction that runs in parallel with the main transaction to improve performance.",
      "A transaction that is executed to undo the effects of a previously committed local transaction within a Saga, effectively providing backward recovery.",
      "A transaction that is used to retry a failed step in a Saga.",
      "A special kind of transaction that can commit even if the coordinator fails."
    ],
    "answer": 1,
    "explanation": "The Saga pattern uses compensating transactions to undo the effects of steps that have already committed, when a later step fails. This provides atomicity across the Saga without locks. Option 0 is not accurate. Option 2 is retry, not compensation. Option 3 is not defined."
  },
  {
    "id": 82,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "You are designing an e-commerce system that must handle high throughput of orders. You decide to use the Saga pattern instead of distributed transactions with 2PC. Which of the following is a potential drawback of this choice?",
    "options": [
      "Sagas cannot guarantee atomicity; if a step fails, the system may be left in an inconsistent state.",
      "Sagas require holding locks for the duration of the entire workflow, reducing concurrency.",
      "Sagas increase the complexity of application logic because developers must write compensating transactions and handle eventual consistency.",
      "Sagas are not suitable for long-running workflows because they block resources."
    ],
    "answer": 2,
    "explanation": "The Saga pattern shifts complexity to the application: you must define compensating transactions, handle failures, and manage eventual consistency. It does guarantee atomicity (via compensation) if properly designed, so option 0 is false. It does not hold locks, so option 1 is false. It is suitable for long-running workflows (option 3 is false)."
  },
  {
    "id": 83,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using optimistic concurrency control (OCC), a transaction reads an item, then later during validation, it discovers that another transaction has committed an update to the same item. What happens?",
    "options": [
      "The transaction is allowed to commit anyway, and the conflict is resolved by merging the updates.",
      "The transaction is aborted and must be retried.",
      "The other transaction is aborted to resolve the conflict.",
      "The system uses a lock to prevent further conflicts."
    ],
    "answer": 1,
    "explanation": "In OCC, if validation fails (i.e., the read set has been modified by committed transactions), the transaction is aborted. The application can then retry it. Option 0 would lead to lost updates. Option 2 is not typical; the other transaction is already committed. Option 3 defeats the purpose of OCC."
  },
  {
    "id": 84,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A system is designed to be PA/EL according to PACELC. Which of the following behaviors would you expect during a network partition?",
    "options": [
      "The system continues to accept writes in all partitions, potentially leading to conflicts that are later resolved.",
      "The system stops accepting writes in some partitions to maintain consistency across the remaining nodes.",
      "The system sacrifices availability to ensure that all nodes have consistent data.",
      "The system routes all requests to a single partition and ignores the others."
    ],
    "answer": 0,
    "explanation": "PA/EL means during a Partition, choose Availability. So the system remains available in all partitions, accepting writes even if they conflict. Option 1 describes PC/EC behavior. Option 2 describes PC/EL or PC/EC. Option 3 is not typical; it might be a form of CP."
  },
  {
    "id": 85,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "Two events have vector timestamps: E1 = [2,1,3], E2 = [2,2,2]. What is the relationship?",
    "options": [
      "E1 happened before E2",
      "E2 happened before E1",
      "E1 and E2 are concurrent",
      "Insufficient information to determine"
    ],
    "answer": 2,
    "explanation": "Compare E1 <= E2: 2 <= 2 (true), 1 <= 2 (true), 3 <= 2 (false). So not <=. Compare E2 <= E1: 2 <= 2 (true), 2 <= 1 (false). So not <=. Hence concurrent."
  },
  {
    "id": 86,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "What is the primary advantage of using hybrid logical clocks (HLC) over vector clocks in a large-scale geo-distributed database?",
    "options": [
      "HLCs provide a total order of events, while vector clocks provide only a partial order.",
      "HLCs have a fixed size (e.g., a 64-bit timestamp) independent of the number of nodes, making them more scalable.",
      "HLCs do not require any message passing to maintain causality.",
      "HLCs can detect concurrent events more accurately than vector clocks."
    ],
    "answer": 1,
    "explanation": "Vector clocks require O(N) space, which becomes problematic in large systems. HLCs combine physical time with a logical component, resulting in a fixed-size timestamp (e.g., (physical time, logical counter)). They scale much better, though they may not capture causality as precisely as vector clocks in some cases. Option 0 is false; both provide partial orders. Option 2 is false; they still need piggybacking. Option 3 is false; they are less precise."
  },
  {
    "id": 87,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "Amazon Dynamo uses vector clocks for conflict detection in shopping carts. If a customer adds an item from one device and removes an item from another device concurrently, resulting in two versions, how does Dynamo typically recommend resolving the conflict?",
    "options": [
      "Dynamo automatically merges the carts using a last-write-wins policy based on timestamps.",
      "Dynamo returns both versions to the application, which must merge them (e.g., taking the union of items, as removal may be interpreted as a side effect).",
      "Dynamo blocks one of the updates, preventing the conflict.",
      "Dynamo uses a consensus algorithm to decide the final state."
    ],
    "answer": 1,
    "explanation": "Dynamo's design philosophy is to push conflict resolution to the application layer because different applications have different merge semantics. For a shopping cart, a common merge is union, but the application decides. Option 0 is a possible configuration but not the recommended default for all applications. Option 2 and 3 are not how Dynamo handles concurrent writes."
  },
  {
    "id": 88,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In MapReduce, what is the purpose of a 'combiner' function?",
    "options": [
      "To combine the outputs of multiple reduce tasks into a single output file.",
      "To perform a local reduce on the map output before the shuffle phase, reducing the amount of data transferred to reducers.",
      "To combine multiple input files into a single split for a map task.",
      "To combine the results of map tasks that run on the same node."
    ],
    "answer": 1,
    "explanation": "A combiner is an optimization that runs on the map side after the map function, performing a mini-reduce (e.g., summing counts) to reduce the volume of intermediate data that must be shuffled across the network. It is essentially a local reduce. Option 0 is not a combiner; that's the job of the reduce phase or output committer. Option 2 is about input splits. Option 3 is close but not precise; it combines map outputs per key, not just per node."
  },
  {
    "id": 89,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "Spark claims to be up to 100x faster than MapReduce for iterative algorithms. What is the primary reason for this speedup?",
    "options": [
      "Spark uses a more efficient programming language (Scala) than MapReduce (Java).",
      "Spark can cache intermediate data in memory across iterations, avoiding the disk writes and reads required by MapReduce for each iteration.",
      "Spark uses a simpler shuffle algorithm that is always faster.",
      "Spark compresses data more effectively than MapReduce."
    ],
    "answer": 1,
    "explanation": "MapReduce writes intermediate results to disk after each map and reduce phase. For iterative algorithms (like machine learning), this disk I/O is a huge bottleneck. Spark's in-memory computing, with the ability to cache RDDs, avoids this, leading to dramatic speedups for iterative workloads. Option 0 is not the main reason. Option 2 is not generally true. Option 3 is a secondary factor."
  },
  {
    "id": 90,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is a 'wide transformation' (e.g., groupByKey, reduceByKey) and why does it cause a stage boundary?",
    "options": [
      "It is a transformation that processes a large amount of data; it causes a stage boundary to free up memory.",
      "It is a transformation that requires data to be shuffled across partitions, meaning that the output of one stage depends on all partitions of the previous stage, necessitating a barrier.",
      "It is a transformation that is applied after an action; it causes a stage boundary to mark the end of lazy evaluation.",
      "It is a transformation that cannot be pipelined; it causes a stage boundary to allow for fault tolerance."
    ],
    "answer": 1,
    "explanation": "Wide transformations (e.g., groupByKey, reduceByKey, join) require data to be repartitioned across the cluster, causing a shuffle. This creates a stage boundary because the next stage cannot start until all data from the previous stage has been shuffled and is available. Narrow transformations (e.g., map, filter) can be pipelined within a single stage."
  },
  {
    "id": 91,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "You are tuning a Spark job and notice that many tasks are failing with 'out of memory' errors. Which of the following adjustments is LEAST likely to help?",
    "options": [
      "Increase the executor memory (spark.executor.memory).",
      "Increase the number of partitions (repartition) to reduce the amount of data per task.",
      "Switch from using groupByKey to reduceByKey, which performs a local combine before shuffle.",
      "Increase the replication factor of the input data in HDFS."
    ],
    "answer": 3,
    "explanation": "Increasing the replication factor in HDFS improves data durability and read locality, but does not help with executor memory errors. The other options directly address memory usage: more memory per executor, less data per partition, or reducing shuffle data size."
  },
  {
    "id": 92,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A system uses a distributed hash table (DHT) for peer-to-peer lookup. Which type of scalability does this architecture primarily address?",
    "options": [
      "Size scalability, because it can handle more nodes and data by distributing the hash space.",
      "Geographical scalability, because DHTs inherently route around network latency.",
      "Administrative scalability, because each node is in its own administrative domain.",
      "All of the above equally."
    ],
    "answer": 0,
    "explanation": "DHTs are designed to scale with the number of nodes (size scalability) by distributing data and lookup responsibility. They do not inherently address geographical latency or administrative domains, though they can be adapted. Option 0 is the most direct."
  },
  {
    "id": 93,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "Which of the following is an example of a system that aims for 'access transparency'?",
    "options": [
      "A database replication system where clients can read from any replica and get the same result.",
      "A remote procedure call (RPC) framework where invoking a method on a remote object looks like a local call.",
      "A load balancer that distributes requests across multiple servers.",
      "A caching system that stores copies of data closer to users."
    ],
    "answer": 1,
    "explanation": "Access transparency hides differences in data representation and invocation mechanisms, making remote calls appear local. RPC is the classic example. Option 0 is replication transparency. Option 2 is not a transparency type. Option 3 is location/migration transparency."
  },
  {
    "id": 94,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "In the context of RPC semantics, what does 'exactly-once' typically require in practice?",
    "options": [
      "A guarantee from the network that no packets are lost or duplicated.",
      "A combination of at-most-once delivery and at-least-once processing.",
      "A system with idempotent operations and duplicate detection, often implemented via a transactional coordinator or exactly-once source semantics (like Kafka's exactly-once).",
      "A synchronous request-response pattern with no retries."
    ],
    "answer": 2,
    "explanation": "Exactly-once semantics are achieved by ensuring that operations are idempotent and that duplicates are detected and suppressed, often using a coordinator that tracks request IDs and outcomes. In streaming systems like Kafka, exactly-once requires idempotent producers, transactional coordination, and idempotent consumers. Option 1 is a combination that could lead to duplicates or losses. Option 0 is unrealistic. Option 3 would be at-most-once."
  },
  {
    "id": 95,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing a notification service that sends emails to users. The service reads from a queue. Which delivery semantic is most appropriate and why?",
    "options": [
      "At-most-once, because sending duplicate emails is annoying and should be avoided, and missing an occasional email is acceptable.",
      "At-least-once, because missing an email is worse than sending a duplicate, and email systems are typically idempotent (sending the same email twice is okay).",
      "Exactly-once, because emails are critical and must be sent exactly once.",
      "It doesn't matter; the email protocol will handle duplicates."
    ],
    "answer": 1,
    "explanation": "In many systems, missing an important email (e.g., password reset) is worse than receiving a duplicate. Email delivery is inherently at-least-once (SMTP may retry). Duplicates are often acceptable (user may get two copies). So at-least-once is common. Option 0 would risk missing emails. Option 2 is overkill and hard to achieve. Option 3 is false; email protocols do not deduplicate."
  },
  {
    "id": 96,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, what is the purpose of marker messages?",
    "options": [
      "To indicate that a process has crashed and needs to be restarted.",
      "To delimit the messages that should be included in the snapshot and to coordinate the recording of local states.",
      "To collect garbage from old snapshots.",
      "To synchronize the clocks of all processes."
    ],
    "answer": 1,
    "explanation": "Markers are used to initiate snapshot recording and to separate messages that are part of the snapshot from those that are not. When a process receives a marker, it knows it must record its state and start recording on other channels. Option 0 is about failure detection. Option 2 is not correct. Option 3 is about clock synchronization."
  },
  {
    "id": 97,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Given vector clocks, if VC(A) = [2,0,0] and VC(B) = [1,2,0], which of the following is true?",
    "options": [
      "A happened before B",
      "B happened before A",
      "A and B are concurrent",
      "A and B are the same event"
    ],
    "answer": 2,
    "explanation": "Compare A <= B: 2 <= 1? false. Compare B <= A: 1 <= 2 true, 2 <= 0 false. So concurrent."
  },
  {
    "id": 98,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In a quorum-based system with N=3, R=2, W=2, what is the maximum number of replica failures that can be tolerated while still allowing both reads and writes to complete?",
    "options": ["0", "1", "2", "3"],
    "answer": 1,
    "explanation": "With N=3, R=2, W=2, a write requires 2 replicas, a read requires 2. If one replica fails, the remaining 2 can still satisfy both R and W. If two replicas fail, only one remains, which cannot satisfy R=2 or W=2. So maximum tolerated failures = 1."
  },
  {
    "id": 99,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "Under sequential consistency, is the following execution possible? P1: write x = 1; P2: read x returns 0, then later read x returns 1; P3: read x returns 1, then later read x returns 0.",
    "options": [
      "Yes, because sequential consistency only requires that each process's own order is preserved, and different processes may see writes at different times.",
      "No, because sequential consistency requires that all processes see the same total order of writes. If P3 sees x=1 then 0, that would imply a write of 0 after 1, which contradicts the order seen by P2.",
      "Yes, because P3's reads could be from a stale cache.",
      "No, because P2's first read should never return 0 if a write happened."
    ],
    "answer": 1,
    "explanation": "Sequential consistency requires a single total order of all operations that respects each process's program order. If P2 sees 1 after 0, the total order must have write(x=1) before the second read. For P3 to see 1 then 0, the total order would have to have write(x=1) then later a write(x=0) that P2 never saw, but there is no such write. So it's impossible."
  },
  {
    "id": 100,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses a failure detector with 'eventual strong completeness' and 'eventual strong accuracy'. What does this mean?",
    "options": [
      "Eventually, every crashed process is permanently suspected by all correct processes, and no correct process is ever suspected.",
      "Eventually, every crashed process is suspected by some correct process, and no correct process is suspected after some time.",
      "Eventually, every crashed process is suspected by all correct processes, and after some time, no correct process is suspected.",
      "Eventually, some crashed processes may never be suspected, but correct processes are never suspected."
    ],
    "answer": 2,
    "explanation": "Eventual strong completeness: every crashed process is eventually suspected by all correct processes. Eventual strong accuracy: there is a time after which no correct process is suspected. Option 0 describes perfect failure detector. Option 1 mixes weak completeness. Option 3 is weak completeness + eventual accuracy."
  },
  {
    "id": 101,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "In the context of view change protocols, what is the purpose of a 'stable storage' for the view information?",
    "options": [
      "To allow processes to recover after a crash and rejoin with the correct view, avoiding split-brain scenarios.",
      "To store application data for durability.",
      "To cache the most recent messages for fast replay.",
      "To log all membership changes for auditing."
    ],
    "answer": 0,
    "explanation": "When a process crashes and recovers, it may have missed view changes. By persisting view information (e.g., the last view it participated in) to stable storage, it can recover with the correct view and not accidentally act on an outdated membership, which could cause inconsistencies. Option 1 is too general. Option 2 and 3 are secondary."
  },
  {
    "id": 102,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Two-Phase Commit (2PC) protocol, what happens if a participant votes ABORT?",
    "options": [
      "The coordinator sends GLOBAL-ABORT to all participants immediately.",
      "The coordinator waits for all votes, then sends GLOBAL-ABORT.",
      "The participant that voted ABORT unilaterally aborts, and the coordinator ignores it.",
      "The coordinator tries to convince the participant to change its vote to COMMIT."
    ],
    "answer": 0,
    "explanation": "In 2PC, once the coordinator receives a single ABORT vote, it can decide to abort immediately. It sends GLOBAL-ABORT to all participants. Some descriptions say it waits for all votes to ensure everyone has voted, but logically it can decide early. However, the standard description: after collecting votes, if any ABORT, send GLOBAL-ABORT. Option 1 is also true in the sense that it waits for all votes to know the outcome, but the decision is made after all votes are in. Both 0 and 1 are plausible, but 0 is more direct about the consequence. Typically, the coordinator waits for all votes to know the outcome, then sends GLOBAL-COMMIT or GLOBAL-ABORT. So option 1 is more accurate: after collecting all votes, it sends GLOBAL-ABORT. Option 0 says 'immediately' which is not strictly accurate; it must receive all votes first. So answer 1 is correct."
  },
  {
    "id": 103,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "A 5-node Raft cluster has a leader. The leader receives a client request, appends it to its log, and sends AppendEntries RPCs to followers. It receives acknowledgments from 2 out of 4 followers (plus itself makes 3). Before it can commit the entry and reply to the client, the leader crashes. What happens to that log entry?",
    "options": [
      "The entry is lost forever because it was not committed.",
      "The entry may be committed by a future leader if it is present on a majority of servers.",
      "The entry is automatically committed because it was replicated to a majority.",
      "The entry will be overwritten by the new leader's log."
    ],
    "answer": 1,
    "explanation": "In Raft, an entry is considered committed when it is known to be stored on a majority of servers. In this scenario, it is stored on a majority (leader + 2 followers = 3 out of 5), so it is committed. However, the leader crashed before telling the client. The new leader, if it has that entry (which it will, because it's on a majority), will eventually commit it and respond to the client (if the client retries). Option 3 is false; committed entries are never overwritten. Option 0 is false because it is committed. Option 2 is true in the sense that it is committed by the definition, but the leader's crash means the client hasn't received confirmation yet. The most accurate is that it is committed and a new leader will have it."
  },
  {
    "id": 104,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "Why is it difficult to implement a linearizable shared log using 2PC?",
    "options": [
      "Because 2PC cannot handle concurrent requests.",
      "Because 2PC is a blocking protocol; if the coordinator fails, participants may block, preventing progress and potentially violating liveness.",
      "Because 2PC requires a total order of operations, which is hard to achieve.",
      "Because 2PC does not support replication."
    ],
    "answer": 1,
    "explanation": "2PC's blocking nature makes it unsuitable for high-availability systems. If the coordinator fails after preparing participants, they block, holding locks, and the system cannot make progress. Consensus algorithms like Paxos/Raft are non-blocking and designed for replicated logs. Option 2 is not the main issue; 2PC can order transactions with a coordinator. Option 3 is false; 2PC coordinates participants, which can be replicas."
  },
  {
    "id": 105,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using strict two-phase locking (2PL), a transaction acquires a shared lock on item X, then later upgrades it to an exclusive lock. What is this called?",
    "options": [
      "Lock conversion",
      "Lock escalation",
      "Lock promotion",
      "Lock upgrade"
    ],
    "answer": 2,
    "explanation": "In database concurrency control, this is typically called lock upgrade or lock conversion. The common term is lock upgrade. Option 1 (conversion) is also used. Lock escalation usually refers to converting many fine-grained locks into a coarser lock. So lock upgrade is most appropriate. But since the options include both, and the question expects one, we'll pick lock promotion? Actually, in many textbooks, it's called lock conversion or upgrade. Given the options, 'lock promotion' might be the intended term. But let's check: In 2PL, a shared lock can be upgraded to exclusive if no other shared locks exist. This is sometimes called lock conversion. I'll go with 2 (lock promotion) as it's listed. However, I'm not entirely sure. To be safe, I'll note that upgrade is common. But since the options are given, and the question is tricky, I'll provide the most likely answer: Option 2."
  },
  {
    "id": 106,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Consider a Saga for a travel booking: Book flight, book hotel, book car. If booking the car fails, what compensating actions should be taken?",
    "options": [
      "Execute compensating transactions for flight and hotel.",
      "Only execute compensating transaction for hotel, because flight is already confirmed.",
      "Retry the car booking until it succeeds.",
      "Do nothing; the system will reconcile later."
    ],
    "answer": 0,
    "explanation": "Since the car step failed, all previously completed steps (flight and hotel) must be compensated. So flight cancellation and hotel cancellation should be executed (in reverse order). Option 1 is incorrect; flight also needs compensation. Option 2 might be attempted but is not a guarantee. Option 3 leads to inconsistency."
  },
  {
    "id": 107,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In optimistic concurrency control (OCC), what is the 'validation phase' responsible for?",
    "options": [
      "Checking that the transaction's read set has not been modified by other committed transactions since it began.",
      "Acquiring locks on all items the transaction will write.",
      "Writing the transaction's updates to the database.",
      "Ensuring that the transaction's timestamp is unique."
    ],
    "answer": 0,
    "explanation": "The validation phase checks for conflicts: if any item in the transaction's read set was modified by a transaction that committed after this transaction started, then validation fails and the transaction is aborted. Option 1 is typical of 2PL. Option 2 is the write phase. Option 3 is not the main purpose."
  },
  {
    "id": 108,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A system is designed to be PC/EC. During a network partition that isolates a minority of nodes, what happens?",
    "options": [
      "The minority partition continues to accept writes, but reads may return stale data.",
      "The minority partition becomes unavailable for writes to maintain consistency.",
      "All partitions continue to operate normally, and conflicts are resolved later.",
      "The system automatically merges the partitions after recovery."
    ],
    "answer": 1,
    "explanation": "PC/EC means during a Partition, choose Consistency. So the minority partition, which cannot participate in a quorum, will stop accepting writes (or become read-only) to avoid inconsistencies. The majority partition continues with strong consistency. Option 0 describes AP behavior. Option 2 is not possible under PC. Option 3 is a recovery detail, not the behavior during partition."
  },
  {
    "id": 109,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "Two events have vector timestamps: E1 = [3,0,2], E2 = [2,3,1]. Which statement is true?",
    "options": [
      "E1 happened before E2",
      "E2 happened before E1",
      "E1 and E2 are concurrent",
      "E1 and E2 are causally related"
    ],
    "answer": 2,
    "explanation": "Compare E1 <= E2: 3 <= 2? false. Compare E2 <= E1: 2 <= 3 true, 3 <= 0 false. So concurrent."
  },
  {
    "id": 110,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "What is a key limitation of using vector clocks in a system with thousands of nodes?",
    "options": [
      "They cannot detect causality accurately.",
      "The size of the vector grows linearly with the number of nodes, causing high metadata overhead.",
      "They require synchronized physical clocks.",
      "They only work in synchronous systems."
    ],
    "answer": 1,
    "explanation": "Vector clocks have O(N) size, where N is the number of nodes. With thousands of nodes, the metadata becomes large, increasing storage and network overhead. This is a scalability issue. Option 0 is false; they are accurate. Option 2 is false. Option 3 is false."
  },
  {
    "id": 111,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is the difference between `reduceByKey` and `groupByKey`?",
    "options": [
      "`reduceByKey` performs a local combine before shuffle, reducing data transfer, while `groupByKey` shuffles all data without combining.",
      "`groupByKey` is an action, while `reduceByKey` is a transformation.",
      "`reduceByKey` can only be used on paired RDDs, while `groupByKey` can be used on any RDD.",
      "There is no difference; they are aliases."
    ],
    "answer": 0,
    "explanation": "`reduceByKey` applies a reduction function locally on each partition before shuffling, which can significantly reduce the amount of data shuffled. `groupByKey` simply shuffles all key-value pairs, which can be expensive if there are many values per key. Option 1 is false; both are transformations. Option 2 is partially true but not the key difference. Option 3 is false."
  },
  {
    "id": 112,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "You are running a Spark job that reads from a large Parquet table partitioned by date. Your query filters on a specific date. What optimization will Catalyst likely apply?",
    "options": [
      "Broadcast join",
      "Predicate pushdown and partition pruning",
      "Whole-stage code generation",
      "Shuffle elimination"
    ],
    "answer": 1,
    "explanation": "Spark SQL (Catalyst) will push the filter down to the data source and prune partitions, reading only the files for the specified date. This significantly reduces I/O. Option 0 is a join optimization. Option 2 is a general code generation optimization. Option 3 is not directly applicable."
  },
  {
    "id": 113,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark Structured Streaming, what is a 'sink'?",
    "options": [
      "The source of streaming data (e.g., Kafka).",
      "The destination where the output of the streaming query is written (e.g., file, console, Kafka).",
      "The checkpoint location for fault tolerance.",
      "The trigger interval for micro-batches."
    ],
    "answer": 1,
    "explanation": "A sink is the output destination of a streaming query. Options 0 is source. Option 2 is checkpoint. Option 3 is trigger."
  },
  {
    "id": 114,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "Which of the following is an example of 'location transparency'?",
    "options": [
      "A user accesses a file using the same pathname regardless of which server stores it.",
      "A web server continues to serve requests even after a hardware failure.",
      "A mobile app switches from Wi-Fi to cellular without dropping the connection.",
      "A database replica is added without changing the application's connection string."
    ],
    "answer": 0,
    "explanation": "Location transparency means that the user (or application) does not need to know the physical location of a resource; it is accessed by a logical name. Option 1 is failure transparency. Option 2 is migration transparency. Option 3 is replication transparency."
  },
  {
    "id": 115,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing a system that sends commands to a remote device. The device may be offline temporarily. You want to ensure that commands are eventually executed, but you don't want to block the sender. Which communication paradigm is most appropriate?",
    "options": [
      "Synchronous RPC with retries",
      "Asynchronous messaging with a persistent queue",
      "Publish-subscribe with transient subscriptions",
      "Raw UDP sockets"
    ],
    "answer": 1,
    "explanation": "A persistent queue will store messages if the device is offline and deliver them when it reconnects. This decouples the sender and provides reliability. Option 0 would block the sender. Option 2 might lose messages if the device is offline. Option 3 is unreliable."
  },
  {
    "id": 116,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "What is the main advantage of using a circuit breaker pattern in inter-service communication?",
    "options": [
      "It encrypts all communication between services.",
      "It prevents a service from repeatedly trying to call a failing service, allowing it to fail fast and avoid cascading failures.",
      "It balances load across multiple instances of a service.",
      "It provides a backup communication channel if the primary fails."
    ],
    "answer": 1,
    "explanation": "A circuit breaker trips when failures reach a threshold, preventing further calls and giving the failing service time to recover. This avoids wasting resources and cascading failures. Option 0 is security. Option 2 is load balancing. Option 3 is about redundancy."
  },
  {
    "id": 117,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, after a process has recorded its local state and sent markers, how does it determine the state of a channel on which it later receives a marker?",
    "options": [
      "It records all messages received on that channel after its local state was recorded and before the marker arrives.",
      "It records the channel state as empty.",
      "It ignores the channel because the snapshot is already complete.",
      "It asks the sender for the list of messages sent."
    ],
    "answer": 0,
    "explanation": "For channels other than the one on which the first marker was received, the process starts recording all incoming messages after it has recorded its local state. When a marker later arrives on that channel, the recorded messages constitute the channel's state. Option 1 is for the channel on which the first marker was received. Option 2 is incorrect. Option 3 is not how it works."
  },
  {
    "id": 118,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "If two events have Lamport timestamps 5 and 7, can we conclude that the event with timestamp 5 happened before the event with timestamp 7?",
    "options": [
      "Yes, because Lamport clocks preserve causality.",
      "No, because Lamport timestamps only guarantee that if A → B then L(A) < L(B), but the converse is not true.",
      "Yes, because timestamps increase monotonically.",
      "No, because the events could be from different processes and the clocks might be unsynchronized."
    ],
    "answer": 1,
    "explanation": "Lamport clocks do not imply causality from timestamp order. They only guarantee that causality implies timestamp order, not vice versa. So 5 < 7 does not imply happened-before."
  },
  {
    "id": 119,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In primary-backup replication, if the primary fails and a backup is promoted, what is the risk if the old primary was partitioned but not actually dead, and later rejoins?",
    "options": [
      "The old primary may have stale data and need to be synchronized.",
      "A split-brain scenario where both act as primary.",
      "Data loss because the old primary's writes were not replicated.",
      "Network congestion due to duplicate messages."
    ],
    "answer": 1,
    "explanation": "If the old primary was isolated in a partition but still accepting writes (if not configured to step down), and a new primary is elected in the other partition, when the partition heals, there will be two nodes claiming to be primary, leading to split-brain. Proper failover mechanisms (like using a lease or consensus) prevent this. Option 0 is a consequence but the primary risk is split-brain."
  },
  {
    "id": 120,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "A quorum-based system with N=5, R=3, W=3 has a write quorum of 3. If a write is acknowledged by 3 replicas, can a subsequent read with R=3 ever return a stale value?",
    "options": [
      "Yes, if the read quorum does not overlap with the write quorum due to failures.",
      "No, because R+W > N guarantees that any read quorum will include at least one replica that has the latest write.",
      "Yes, if the read occurs before the write is fully propagated.",
      "No, because all writes are synchronous."
    ],
    "answer": 1,
    "explanation": "With R+W > N (3+3=6 > 5), any read quorum must intersect any write quorum. Therefore, any successful read will see at least one replica that participated in the latest write quorum, thus seeing the latest value (assuming version numbers). Option 0 is impossible because intersection is guaranteed. Option 2 is not applicable because the write is considered durable only after 3 acks; a read after that will see it due to overlap."
  },
  {
    "id": 121,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A distributed system uses a gossip protocol for failure detection. What is a key advantage of gossip over centralized heartbeat monitoring?",
    "options": [
      "Gossip is faster at detecting failures.",
      "Gossip scales better because there is no single point of failure or bottleneck, and each node only communicates with a few peers.",
      "Gossip guarantees that all nodes agree on the failure simultaneously.",
      "Gossip uses less network bandwidth overall."
    ],
    "answer": 1,
    "explanation": "Gossip protocols are decentralized and scalable. Each node exchanges information with a small set of peers periodically, so the load is distributed. Option 0 is not necessarily true; gossip may be slower. Option 2 is false; agreement is not simultaneous. Option 3 is not necessarily true; gossip can generate more total messages but spreads load."
  },
  {
    "id": 122,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "In a system using write-ahead logging (WAL), what is the purpose of the 'commit' record in the log?",
    "options": [
      "To indicate that the transaction's changes are durable and the transaction is complete.",
      "To mark the beginning of a transaction.",
      "To undo the effects of a transaction in case of abort.",
      "To provide a checkpoint for recovery."
    ],
    "answer": 0,
    "explanation": "The commit record in the WAL signifies that the transaction has committed and all its changes are durable. During recovery, the system will redo all transactions that have a commit record. Option 1 is begin record. Option 2 is about undo logs. Option 3 is not the main purpose."
  },
  {
    "id": 123,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Paxos algorithm, what does an acceptor do when it receives a PREPARE request with proposal number n, and it has already promised not to accept any proposal with number less than n?",
    "options": [
      "It rejects the request.",
      "It promises not to accept any future proposals with number less than n and returns the highest-numbered proposal it has already accepted (if any).",
      "It accepts the proposal immediately.",
      "It forwards the request to the leader."
    ],
    "answer": 1,
    "explanation": "In Paxos phase 1, an acceptor, upon receiving a PREPARE with number n greater than any it has seen, promises to ignore future PREPAREs with lower numbers and responds with the highest-numbered proposal it has already accepted (if any). This is how Paxos ensures safety."
  },
  {
    "id": 124,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, what happens if a follower receives an AppendEntries RPC with a term number that is less than its current term?",
    "options": [
      "It accepts the entries and updates its term.",
      "It rejects the RPC and returns its current term, causing the sender to step down if it's a leader.",
      "It ignores the RPC and does nothing.",
      "It forwards the request to the current leader."
    ],
    "answer": 1,
    "explanation": "If a node receives a request with a stale term, it rejects it and returns its current term. This informs the sender (which might be a leader from an older term) that it is outdated, causing it to step down. Option 0 is wrong because term must be >= current. Option 2 is not correct. Option 3 is not defined."
  },
  {
    "id": 125,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Consider a distributed transaction using 2PC. The coordinator sends PREPARE to all participants. One participant, P1, votes ABORT. The coordinator logs this and decides to abort. Before it can send GLOBAL-ABORT to any participant, the coordinator crashes. What is the state of the other participants who have not yet received any decision?",
    "options": [
      "They are in the prepared state, waiting for the coordinator.",
      "They will eventually abort after a timeout because they haven't received a decision.",
      "They will commit because they voted COMMIT.",
      "They are blocked indefinitely."
    ],
    "answer": 1,
    "explanation": "In 2PC, participants that have not yet received a decision will eventually timeout and abort. This is a safety measure to prevent indefinite blocking. However, note that if they had already voted COMMIT, they are in prepared state and cannot unilaterally abort without risking inconsistency. In this scenario, since the coordinator hasn't sent any decision, participants are still waiting; they may have timeouts that cause them to abort. But typical 2PC implementations have timeouts to avoid blocking. So option 1 is most plausible. Option 3 is the blocking problem but with timeouts it's mitigated. So answer is 1."
  },
  {
    "id": 126,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In the context of distributed deadlock detection, what is a 'wait-for graph'?",
    "options": [
      "A graph showing which transactions are waiting for which locks.",
      "A graph showing the flow of messages between processes.",
      "A graph representing the dependencies in a distributed computation.",
      "A graph used to detect cycles that indicate deadlock."
    ],
    "answer": 3,
    "explanation": "A wait-for graph has nodes representing transactions and edges representing 'waiting for'. A cycle in this graph indicates a deadlock. Option 0 is essentially the definition, but option 3 is the purpose. Both are correct, but 3 directly answers the question. However, 0 is also correct. Typically, wait-for graph is defined as a directed graph where nodes are transactions and an edge from T1 to T2 means T1 is waiting for a lock held by T2. So option 0 is the definition. Option 3 is the consequence. The question asks 'what is a wait-for graph', so definition is appropriate. I'll go with 0."
  },
  {
    "id": 127,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A system uses causal consistency. Which of the following anomalies is possible?",
    "options": [
      "A user sees a reply to a message before seeing the original message.",
      "Two users see updates to independent items in different orders.",
      "A user sees a write, then a read that returns an older value (stale read).",
      "A user's update is lost due to a concurrent update from another user."
    ],
    "answer": 1,
    "explanation": "Causal consistency allows concurrent updates to be seen in different orders, as long as causally related ones are ordered. Option 0 is a causal violation. Option 2 is possible under eventual consistency but causal consistency typically provides monotonic reads? Actually, causal consistency may allow stale reads if not combined with other guarantees. But the classic definition: causal consistency ensures that causally related operations are seen in order by all processes. Concurrent operations can be seen in any order. So option 1 is allowed. Option 3 is about lost updates, which can happen under some consistency models but not necessarily causal."
  },
  {
    "id": 128,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "What is the main advantage of using consistent hashing in a distributed key-value store like Dynamo or Cassandra?",
    "options": [
      "It provides strong consistency guarantees.",
      "It minimizes data movement when nodes are added or removed.",
      "It ensures that all replicas are updated synchronously.",
      "It enables complex querying like joins."
    ],
    "answer": 1,
    "explanation": "Consistent hashing assigns keys to nodes in a way that when a node is added or removed, only a small fraction of keys need to be remapped. This is crucial for scalability and availability in dynamic environments. Option 0 is not related; consistent hashing is about partitioning, not consistency. Option 2 is about replication. Option 3 is not a feature."
  },
  {
    "id": 129,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In MapReduce, what is the role of the 'partitioner'?",
    "options": [
      "To split the input data into chunks for map tasks.",
      "To determine which reducer receives each intermediate key, controlling load balancing.",
      "To combine map outputs locally before shuffle.",
      "To sort the keys before reducing."
    ],
    "answer": 1,
    "explanation": "The partitioner decides, for each key, which reducer will process it. The default is hash partitioning, but custom partitioners can be used for load balancing or logical grouping. Option 0 is input format. Option 2 is combiner. Option 3 is part of shuffle/sort."
  },
  {
    "id": 130,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is a 'DataFrame' and how does it differ from an RDD?",
    "options": [
      "A DataFrame is a distributed collection of objects, while an RDD is a distributed collection of rows.",
      "A DataFrame is a distributed collection of rows with a schema, allowing Catalyst optimizations, while an RDD is a lower-level API with no schema.",
      "A DataFrame is an RDD of Row objects, so they are essentially the same.",
      "A DataFrame is a Python-specific API, while RDD is Scala-only."
    ],
    "answer": 1,
    "explanation": "DataFrames have a schema and are built on top of RDDs, but they enable Catalyst optimizations and Tungsten execution. RDDs are more flexible but lack these optimizations. Option 0 is backwards. Option 2 is incorrect; DataFrames are more than just RDD[Row]. Option 3 is false."
  },
  {
    "id": 131,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "Which of the following is NOT a characteristic of a decentralized system as defined in the lecture?",
    "options": [
      "Processes and resources are necessarily spread across multiple computers.",
      "There is no central coordinator.",
      "All nodes are equal and no node has a global view.",
      "The system can be managed by a single administrator."
    ],
    "answer": 3,
    "explanation": "The lecture defined a decentralized system as one where resources are necessarily spread across multiple computers, but it didn't say there is no central coordinator (e.g., blockchain has no central authority but many decentralized systems have logical coordinators). Option 3 is not a characteristic; decentralized systems often span multiple administrative domains, so they are not managed by a single administrator. Option 2 is a common property of pure P2P systems but not all decentralized systems. However, the question asks 'NOT a characteristic'. The lecture explicitly mentions 'necessarily spread' as definitional. Option 3 is a fallacy, not a characteristic."
  },
  {
    "id": 132,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A developer argues that using a message queue with exactly-once semantics eliminates the need for idempotent consumers. Is this correct?",
    "options": [
      "Yes, exactly-once delivery guarantees that each message is processed exactly once.",
      "No, because exactly-once delivery only ensures that the message is delivered exactly once by the broker, but the consumer may still fail after processing and before acknowledging, leading to at-least-once processing.",
      "Yes, because exactly-once includes processing semantics.",
      "No, because exactly-once is impossible to achieve in practice."
    ],
    "answer": 1,
    "explanation": "Exactly-once delivery from the broker means the message will not be lost or duplicated by the broker. However, the consumer may still crash after processing but before committing its offset, causing the message to be redelivered. To achieve end-to-end exactly-once, the consumer must be idempotent or use a transactional approach. Option 2 is false; exactly-once delivery is about delivery, not processing. Option 4 is too absolute."
  },
  {
    "id": 133,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "What is the purpose of adding jitter to exponential backoff in retry mechanisms?",
    "options": [
      "To make the retry intervals unpredictable, preventing synchronized retries from multiple clients.",
      "To reduce the average retry time.",
      "To ensure that retries happen at increasing intervals.",
      "To guarantee that a retry eventually succeeds."
    ],
    "answer": 0,
    "explanation": "Jitter introduces randomness to break synchronization. Without jitter, many clients retrying at the same exponential intervals could create thundering herd problems. Option 1 is not the purpose; jitter may increase average time. Option 2 is exponential backoff itself. Option 3 is not guaranteed."
  },
  {
    "id": 134,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Consider a distributed system with three processes. Process P1 has a local event and then sends a message to P2. P2 receives the message and then sends a message to P3. P3 receives the message. Using Lamport clocks (initialized to 0), what is the Lamport timestamp of the event at P3 when it receives the message?",
    "options": ["1", "2", "3", "4"],
    "answer": 2,
    "explanation": "Let's simulate: P1 local event: L=1. Send message with timestamp 1. P2 receives: max(L2, 1)+1. Assume L2 started at 0, so L2 becomes max(0,1)+1=2. Then P2 sends to P3 with timestamp 2. P3 receives: max(L3,2)+1. L3 starts at 0, so L3 becomes max(0,2)+1=3. So event at P3 has timestamp 3. Option 3 is correct."
  },
  {
    "id": 135,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Which of the following is true about vector clocks?",
    "options": [
      "They can be used to totally order events.",
      "They can detect causal relationships but not concurrency.",
      "They can detect both causal relationships and concurrency.",
      "They require synchronized physical clocks."
    ],
    "answer": 2,
    "explanation": "Vector clocks can determine if A → B, B → A, or if they are concurrent. Option 0 is false; they provide partial order. Option 1 is false; they detect concurrency. Option 3 is false."
  },
  {
    "id": 136,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In primary-backup replication, if the primary fails and a backup is promoted, how can the new primary ensure it has all committed updates?",
    "options": [
      "It assumes its log is up-to-date because it received all writes from the old primary.",
      "It performs a reconciliation process with all other backups to find the most recent updates.",
      "It must have been receiving all writes synchronously, so its state is identical to the primary's at the time of failure.",
      "It requests a state transfer from the old primary, which may still be running."
    ],
    "answer": 2,
    "explanation": "In primary-backup with synchronous replication, the backup is updated before the write is acknowledged, so it has all committed writes. In asynchronous replication, the backup may be behind. The question implies a standard primary-backup with sync replication, so option 2 is correct. Option 1 is not guaranteed if replication is async. Option 3 is impossible if primary crashed. Option 4 is possible only if primary recovers, but usually we promote a backup."
  },
  {
    "id": 137,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "A quorum-based system with N=5, R=2, W=4 is used. What is the minimum number of replica failures that can cause a read to fail?",
    "options": ["1", "2", "3", "4"],
    "answer": 2,
    "explanation": "Read quorum is 2, so if 3 replicas are available, read can succeed. If 3 replicas fail, only 2 remain, which can still satisfy R=2? Actually, if 3 fail, 2 remain, so read can still succeed because R=2 can be satisfied by those 2. So read can tolerate up to 3 failures. The question asks minimum that can cause a read to fail: if 4 replicas fail, only 1 remains, which is insufficient for R=2. So minimum is 4. But check options: 4 is an option. So answer is 4. However, the question might be tricky: with R=2, you need at least 2 replicas. So if 4 fail, only 1 left, read fails. So answer 4."
  },
  {
    "id": 138,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses pessimistic message logging for rollback recovery. What does this involve?",
    "options": [
      "Logging each message before it is processed, so that in case of failure, the process can replay messages in the same order.",
      "Logging messages asynchronously to avoid blocking.",
      "Checkpointing the state periodically and logging messages after processing.",
      "Using vector clocks to track causal dependencies."
    ],
    "answer": 0,
    "explanation": "Pessimistic message logging logs each message to stable storage before it is processed. This ensures that if the process fails, it can replay the messages exactly as before, achieving deterministic recovery. Option 1 describes optimistic logging. Option 2 is about asynchronous. Option 3 is not pessimistic logging."
  },
  {
    "id": 139,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "What is a key challenge of using optimistic message logging?",
    "options": [
      "It may cause the domino effect because logs are not guaranteed to be on stable storage.",
      "It requires synchronous logging, which slows down processing.",
      "It cannot recover from failures that occur during logging.",
      "It is only applicable to deterministic processes."
    ],
    "answer": 0,
    "explanation": "Optimistic logging logs asynchronously, so if a failure occurs, some logged messages may be lost, potentially causing a cascade of rollbacks (domino effect). Option 1 is a characteristic of pessimistic logging. Option 2 is not the main challenge; optimistic logging can recover but with more complexity. Option 3 is true for all message logging."
  },
  {
    "id": 140,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Paxos algorithm, what does it mean for a value to be 'chosen'?",
    "options": [
      "A majority of acceptors have accepted the same value in response to an ACCEPT request.",
      "The proposer has sent an ACCEPT request to all acceptors.",
      "All acceptors have promised to accept the value.",
      "The learners have been notified of the value."
    ],
    "answer": 0,
    "explanation": "A value is chosen when a majority of acceptors have accepted it (i.e., responded to an ACCEPT request). This is the definition of consensus in Paxos. Option 1 is not enough; they must accept. Option 2 is promise phase. Option 3 is after learning."
  },
  {
    "id": 141,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, what is the purpose of the 'commit index'?",
    "options": [
      "To track the highest log entry that is known to be stored on a majority of servers and thus can be applied to the state machine.",
      "To indicate the last log entry that has been applied to the state machine.",
      "To mark the point up to which the leader has sent AppendEntries.",
      "To store the index of the last committed entry in the leader's log."
    ],
    "answer": 0,
    "explanation": "The commit index is the index of the highest log entry known to be committed (replicated on a majority). Once an entry's index is ≤ commit index, it can be applied to the state machine. Option 1 is last applied. Option 2 is not defined. Option 3 is not accurate."
  },
  {
    "id": 142,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using timestamp ordering for concurrency control, each transaction is assigned a unique timestamp. If two transactions have timestamps T1 < T2, and they both want to write the same item, how is conflict resolved?",
    "options": [
      "T1's write is allowed, and T2's write is rejected and T2 is aborted (Thomas Write Rule may allow ignoring older writes).",
      "T2's write is allowed, and T1 is aborted because it's older.",
      "Both writes are allowed, and the conflict is resolved at commit time.",
      "The writes are serialized in timestamp order, with T1's write happening before T2's."
    ],
    "answer": 0,
    "explanation": "In basic timestamp ordering, if T1 (older) writes, and then T2 (younger) tries to write, T2 is aborted because its write would come after T1's, violating timestamp order. However, if T1 has already committed, T2's write is rejected. If T1 hasn't committed yet, T2 may wait or abort. Thomas Write Rule allows ignoring outdated writes, but typically the older write wins. Option 0 is most accurate. Option 1 is opposite. Option 3 is not correct because T2 would be aborted."
  },
  {
    "id": 143,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Which of the following is a characteristic of nested transactions?",
    "options": [
      "They require two-phase commit across all subtransactions.",
      "A subtransaction can abort without aborting the parent transaction, allowing the parent to retry or choose an alternative.",
      "They are always executed sequentially.",
      "They do not support concurrency."
    ],
    "answer": 1,
    "explanation": "Nested transactions allow subtransactions to abort independently; the parent can handle the abort and possibly retry. They can run concurrently. Option 0 is not necessarily true; they may use a commit protocol. Option 2 is false. Option 3 is false."
  },
  {
    "id": 144,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A geo-distributed database uses active-active replication with asynchronous cross-region replication. A user in Europe updates their profile. A few seconds later, a user in Asia reads that profile and sees the old version. This is an example of:",
    "options": [
      "A violation of linearizability.",
      "A violation of sequential consistency.",
      "Eventual consistency in action.",
      "A network partition."
    ],
    "answer": 2,
    "explanation": "Asynchronous replication leads to temporary inconsistencies. The system is eventually consistent: after some time, the update will propagate. This is not a violation of a consistency model if the system only guarantees eventual consistency. It is an example of eventual consistency. Option 0 and 1 would be violations if the system claimed those models. Option 3 is not relevant."
  },
  {
    "id": 145,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is the role of the 'DAG Scheduler'?",
    "options": [
      "It converts a logical query plan into a physical plan and optimizes it.",
      "It builds a directed acyclic graph (DAG) of stages and submits each stage as a set of tasks to the Task Scheduler.",
      "It manages memory and storage for RDDs.",
      "It handles fault tolerance by recomputing lost partitions from lineage."
    ],
    "answer": 1,
    "explanation": "The DAG Scheduler takes the RDD lineage and builds stages of tasks, then submits them to the Task Scheduler. Option 0 is Catalyst's job. Option 2 is BlockManager. Option 3 is a feature but not the primary role of DAG Scheduler; it handles fault tolerance by recomputing lost partitions, but that's a consequence of lineage."
  },

  {
    "id": 146,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is the difference between `cache()` and `persist()`?",
    "options": [
      "`cache()` stores data in memory only, while `persist()` can store data in memory, disk, or off-heap with various storage levels.",
      "`cache()` is an action, while `persist()` is a transformation.",
      "`cache()` is for DataFrames, while `persist()` is for RDDs.",
      "There is no difference; they are identical."
    ],
    "answer": 0,
    "explanation": "`cache()` is simply a shorthand for `persist(StorageLevel.MEMORY_ONLY)`. `persist()` allows you to specify different storage levels (e.g., MEMORY_AND_DISK, MEMORY_ONLY_SER, etc.). Option 1 is false; both are transformations. Option 2 is false. Option 3 is false."
  },
  {
    "id": 147,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A distributed system is designed to be 'open'. Which of the following features is most essential for achieving openness?",
    "options": [
      "Using a proprietary communication protocol optimized for performance.",
      "Publishing well-defined interfaces using an Interface Definition Language (IDL) and supporting interoperability.",
      "Centralizing all configuration management to ensure consistency.",
      "Using a single programming language for all components."
    ],
    "answer": 1,
    "explanation": "Openness is about being able to interact with other systems irrespective of the underlying environment. This requires well-defined interfaces, interoperability, and portability. Option 0 is the opposite; proprietary protocols hinder openness. Option 2 is about management, not openness. Option 3 is not necessary; openness should allow multiple languages."
  },
  {
    "id": 148,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "The lecture discusses the 'fallacy that the network is homogeneous'. What does this mean in practice?",
    "options": [
      "Assuming that all network links have the same bandwidth and latency.",
      "Assuming that all devices on the network run the same operating system.",
      "Assuming that all network protocols are compatible with each other.",
      "Assuming that the network is managed by a single entity."
    ],
    "answer": 0,
    "explanation": "The fallacy of homogeneity includes assuming that network performance (bandwidth, latency) is uniform, which is rarely true in real-world networks, especially WANs. Option 1 is about hardware/software homogeneity, which is also a fallacy but not the primary network homogeneity one. The lecture lists 'The network is homogeneous' as a separate fallacy. It typically refers to performance and characteristics being uniform."
  },
  {
    "id": 149,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing a system that requires ordered processing of messages from a single source, but you also need to scale out processing across multiple consumer instances. Which Kafka configuration achieves this?",
    "options": [
      "Use multiple topics, one per consumer.",
      "Use a single partition within a topic, so all messages go to one consumer, but you lose scalability.",
      "Use multiple partitions, and ensure that all messages that need ordering have the same key, so they go to the same partition and thus the same consumer.",
      "Use a consumer group with multiple consumers, and rely on the broker to order across partitions."
    ],
    "answer": 2,
    "explanation": "Kafka guarantees order only within a partition. To scale while maintaining order for related messages, you assign a key (e.g., user ID) so that all messages with that key go to the same partition. Different keys can go to different partitions, allowing parallel processing. Option 1 doesn't help with ordering. Option 3 is false; the broker does not order across partitions."
  },
  {
    "id": 150,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "What is the primary purpose of a 'dead letter queue' (DLQ) in a messaging system?",
    "options": [
      "To store messages that are waiting to be processed due to backpressure.",
      "To store messages that could not be processed successfully after multiple retries, for later inspection or manual intervention.",
      "To act as a backup queue in case the primary queue fails.",
      "To prioritize certain messages over others."
    ],
    "answer": 1,
    "explanation": "A DLQ is used to isolate messages that consumers repeatedly fail to process (poison messages). This prevents them from blocking the main queue and allows for debugging or reprocessing later. Option 0 is about buffering. Option 2 is about redundancy. Option 3 is about priority queues."
  },
  {
    "id": 151,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, suppose a process receives a marker on a channel, and it has already recorded its local state (because it received a marker on another channel first). What should it do?",
    "options": [
      "Record the state of this channel as all messages received after the marker.",
      "Record the state of this channel as all messages received before the marker.",
      "Record the state of this channel as empty, because the marker indicates the end of channel recording.",
      "Ignore the marker, as the snapshot is already complete for this process."
    ],
    "answer": 2,
    "explanation": "If a process has already recorded its state, and then receives a marker on another channel, it should record the state of that channel as the sequence of messages received on that channel since it recorded its state, up until the marker arrives. But if it has already recorded its state, it has been recording messages on all other channels. When the marker arrives, it stops recording for that channel, and the recorded messages constitute the channel's state. The marker itself is not recorded. So the channel state is the messages received after the local state recording started and before the marker. Option 2 says 'empty', which is incorrect; it's the messages received during that interval. Actually, let's recall: In Chandy-Lamport, when a process receives a marker for the first time, it records its local state and starts recording on all other channels. When it later receives a marker on another channel, it stops recording on that channel, and the recorded messages are the channel's state. So the answer should be that it records the messages received since it started recording. Option 0 is closest: 'all messages received after the marker'? No, after the marker would be after the snapshot. It's messages received after local state recording started and before the marker. Option 0 says 'after the marker', which is wrong. Option 1 says 'before the marker' but doesn't specify since when. Option 2 is wrong. This is a tricky one. The correct answer is not listed perfectly. Perhaps option 0 is intended to mean 'after the local state recording started' but it says 'after the marker'. I'll need to pick the closest. In many descriptions, the channel state is the messages received after the local state was recorded and before the marker. So option 1 could be interpreted as that if we assume 'before the marker' means since recording started. But it's ambiguous. I'll go with option 0 because it's the only one that mentions 'after the marker' which is part of the definition (messages received after starting recording and before marker are the ones that are recorded). Actually, the messages received after the marker are not recorded; they are part of the next snapshot. So option 0 is incorrect. This question might be flawed. I'll rephrase in my mind: The correct action is to record the channel as the sequence of messages received on that channel since the process recorded its state, up to the marker. That is not any of the options. Given the options, none are perfectly correct. Perhaps option 1 is intended: 'all messages received before the marker' (since the state recording started, so before the marker means after recording started and before marker). I'll choose option 1."
  },
  {
    "id": 152,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Given vector clocks, if VC(A) = [1,0,0] and VC(B) = [0,1,0], which of the following is true?",
    "options": [
      "A happened before B",
      "B happened before A",
      "A and B are concurrent",
      "A and B are causally related"
    ],
    "answer": 2,
    "explanation": "Compare A <= B: 1 <= 0 false. Compare B <= A: 0 <= 1 true, 1 <= 0 false. So concurrent."
  },
  {
    "id": 153,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "A distributed key-value store uses chain replication with a chain of three nodes: N1 (head), N2, N3 (tail). A write is received by N1, propagated to N2, then to N3. N3 acknowledges to N2, N2 to N1, and N1 to client. If N2 fails after acknowledging to N1 but before propagating to N3, what is the state?",
    "options": [
      "The write is lost because N2 crashed before N3 received it.",
      "The write is safe because N1 has it and N3 will receive it when N2 recovers or via recovery protocol.",
      "The write is safe because N1 acknowledged to client, and N2 had it before crashing, so it will be propagated during recovery.",
      "The write is lost because N1 acknowledged prematurely."
    ],
    "answer": 2,
    "explanation": "In chain replication, once a node has received a write, it persists it. If N2 crashes after having the write, it will still have it when it recovers. The system will need to reconfigure the chain (remove N2) and ensure N3 gets the write from either N1 or the recovered N2. Since N1 has the write and acknowledged, the write is durable. Option 2 is most accurate."
  },
  {
    "id": 154,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In a quorum-based system with N=7, what is the minimum size of read and write quorums (R and W) to ensure that reads always see the latest write, while also maximizing availability (i.e., allowing as many failures as possible)?",
    "options": ["R=4, W=4", "R=3, W=5", "R=5, W=3", "R=7, W=1"],
    "answer": 0,
    "explanation": "To ensure reads see latest writes, we need R+W > N. To maximize availability, we want R and W as small as possible because smaller quorums can tolerate more failures. The smallest integer pair that satisfies R+W > 7 and both ≤7 is R=4, W=4 (since 4+4=8>7). This allows up to 3 failures for reads (need 4 of 7) and up to 3 for writes. Option 1 (3,5) allows more write failures? 3+5=8>7, but W=5 can tolerate 2 failures, R=3 can tolerate 4 failures. So overall, the minimum quorum sizes that are balanced are 4 and 4. Option 2 is symmetric. Option 3 has W=1 which is very small but then R must be 7, which is terrible for read availability. So 4 and 4 is the balanced, high-availability choice."
  },
  {
    "id": 155,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses a group membership protocol with a central coordinator. The coordinator fails. How can the system recover?",
    "options": [
      "The system cannot recover; it must wait for the coordinator to restart.",
      "The remaining nodes elect a new coordinator using a consensus algorithm, and the new coordinator reconstructs the membership view from the other nodes.",
      "Each node independently decides on a new membership and continues, leading to potential inconsistencies.",
      "The system shuts down until manual intervention."
    ],
    "answer": 1,
    "explanation": "To avoid a single point of failure, group membership protocols often use a leader election to choose a new coordinator. The new leader can gather state from the remaining nodes to form a consistent view. Option 0 is not fault-tolerant. Option 2 leads to split-brain. Option 3 is not automated."
  },
  {
    "id": 156,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "What is the purpose of a 'watchdog timer' in fault-tolerant systems?",
    "options": [
      "To measure network latency.",
      "To detect if a process has hung or crashed by expecting a periodic 'I'm alive' signal.",
      "To schedule periodic checkpoints.",
      "To synchronize clocks across nodes."
    ],
    "answer": 1,
    "explanation": "A watchdog timer is used to detect failures. A process must periodically reset the timer; if it expires, the process is assumed to have failed. Option 0 is for performance. Option 2 is for checkpointing. Option 3 is for clock sync."
  },
  {
    "id": 157,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Paxos algorithm, what is the role of the 'leader' (distinguished proposer)?",
    "options": [
      "To act as a coordinator and sequence proposals to avoid conflicts and improve liveness.",
      "To make all decisions unilaterally.",
      "To collect votes and decide the outcome without waiting for acceptors.",
      "To replace the acceptors in the voting process."
    ],
    "answer": 0,
    "explanation": "In Multi-Paxos, a leader is elected to sequence proposals and avoid multiple proposers conflicting, which improves liveness. The leader still must go through the acceptors. Option 1 is false; it's not unilateral. Option 2 is false; it must get majority. Option 3 is false."
  },
  {
    "id": 158,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, what is the purpose of the 'term'?",
    "options": [
      "To act as a logical clock that divides time into epochs, each starting with an election, and helps detect stale leaders.",
      "To measure the duration of a leader's tenure.",
      "To number log entries sequentially.",
      "To prioritize which node should become leader."
    ],
    "answer": 0,
    "explanation": "The term is a monotonically increasing number that acts as a logical clock. Each term begins with an election. If a node receives a request with a higher term, it updates its term; if it receives a lower term, it rejects it. This helps nodes identify stale leaders. Option 1 is a description but not the purpose. Option 2 is log index. Option 3 is a side effect."
  },
  {
    "id": 159,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using strict two-phase locking (2PL), what is the 'growing phase'?",
    "options": [
      "The phase where a transaction acquires locks and cannot release any.",
      "The phase where a transaction releases locks and cannot acquire any new ones.",
      "The phase where a transaction grows by adding new operations.",
      "The phase where the transaction is waiting for locks."
    ],
    "answer": 0,
    "explanation": "In 2PL, the growing phase is when the transaction acquires locks. It cannot release any locks during this phase. The shrinking phase is when it releases locks. Option 1 is shrinking phase. Option 2 is not a term. Option 3 is not accurate."
  },
  {
    "id": 160,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Consider a Saga with three transactions: T1, T2, T3, with compensating transactions C1, C2, C3. If T1 and T2 succeed, but T3 fails, which compensating transactions are executed and in what order?",
    "options": [
      "C3, then C2, then C1",
      "C2, then C1",
      "C1, then C2",
      "C3 only"
    ],
    "answer": 1,
    "explanation": "When T3 fails, the Saga must undo the effects of T2 and T1 (in reverse order). C3 is not executed because T3 failed and may have had no effect or its failure is the trigger. So compensation starts with C2, then C1. Option 0 includes C3 unnecessarily. Option 2 is forward order. Option 3 leaves T2 and T1 effects."
  },
  {
    "id": 161,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A system uses causal consistency. Which of the following scenarios is possible?",
    "options": [
      "User A posts a photo. User B comments on the photo. User C sees the comment before the photo.",
      "User A updates their profile picture. User B sees the old picture after seeing the new one.",
      "User A sends a message to User B. User B replies. User A sees the reply before the original message.",
      "Two users update their statuses independently. A third user sees the updates in different orders on different devices."
    ],
    "answer": 3,
    "explanation": "Causal consistency requires causally related operations to be seen in order. Independent updates (concurrent) can be seen in any order. Option 0 violates causality (comment depends on photo). Option 1 violates monotonic reads if not provided. Option 2 violates causality (reply depends on message). Option 3 is allowed."
  },
  {
    "id": 162,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "What is the main trade-off in using CRDTs (Conflict-free Replicated Data Types) for eventual consistency?",
    "options": [
      "They require complex coordination protocols to merge conflicts.",
      "They have limited data types and operations that can be expressed commutatively, and may have higher storage or computational overhead.",
      "They cannot guarantee convergence.",
      "They require vector clocks for all operations."
    ],
    "answer": 1,
    "explanation": "CRDTs are designed to converge without coordination, but they are limited to data types where operations commute (e.g., counters, sets with add-only). They may also require storing additional metadata (tombstones) to handle removals. Option 0 is false; they avoid coordination. Option 2 is false; they guarantee convergence. Option 3 is false; they may use version vectors but not always."
  },
  {
    "id": 163,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark Structured Streaming, what is a 'trigger'?",
    "options": [
      "A mechanism to start the streaming query.",
      "A setting that defines how often the streaming query is executed, e.g., micro-batch interval or continuous processing.",
      "A function that is called when an event arrives.",
      "A checkpoint location."
    ],
    "answer": 1,
    "explanation": "The trigger specifies the timing of streaming execution: processing time interval, once, or continuous. Option 0 is not specific. Option 2 is a function. Option 3 is checkpoint."
  },
  {
    "id": 164,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "Which of the following is a benefit of using DataFrames over RDDs in Spark?",
    "options": [
      "DataFrames provide a richer, type-safe API.",
      "DataFrames allow Spark to apply Catalyst optimizations and Tungsten execution, improving performance.",
      "DataFrames are easier to use for low-level imperative programming.",
      "DataFrames support exactly-once processing natively."
    ],
    "answer": 1,
    "explanation": "DataFrames have a schema, enabling Catalyst optimizations (predicate pushdown, etc.) and Tungsten code generation, which can significantly improve performance. Option 0 is false; RDDs are more type-safe in Scala. Option 2 is false; RDDs are better for low-level control. Option 3 is not a specific benefit."
  },
  {
    "id": 165,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "A distributed system aims for 'replication transparency'. What does this mean for the user?",
    "options": [
      "The user can choose which replica to access.",
      "The user is unaware that multiple copies of data exist; all replicas appear as one logical object.",
      "The user sees all replicas and can observe inconsistencies.",
      "The user's requests are replicated for fault tolerance."
    ],
    "answer": 1,
    "explanation": "Replication transparency hides the fact that multiple copies exist, so the user interacts with a single logical resource. Option 0 is the opposite. Option 2 is not transparency. Option 3 is about fault tolerance, not transparency."
  },
  {
    "id": 166,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "The lecture mentions that 'centralized solutions have a single point of failure' is often a misconception. Why?",
    "options": [
      "Because centralized solutions can be made highly available through redundancy and failover, even if logically centralized.",
      "Because all distributed systems also have single points of failure.",
      "Because the term 'single point of failure' is meaningless.",
      "Because centralized solutions are always more robust."
    ],
    "answer": 0,
    "explanation": "The lecture points out that a logically centralized component (like DNS root) can be physically distributed and made highly robust, so it is not necessarily a single point of failure. Option 1 is false. Option 2 is false. Option 3 is false."
  },
  {
    "id": 167,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "You are designing an RPC system for a high-performance computing cluster with low-latency, reliable network. Which RPC semantic would you choose and why?",
    "options": [
      "At-most-once, to avoid duplicate computation in case of retries.",
      "At-least-once, because the network is reliable and retries are unlikely.",
      "Exactly-once, because it's the only correct semantic.",
      "Maybe-once, because performance is critical."
    ],
    "answer": 0,
    "explanation": "In a reliable, low-latency environment, you can use at-most-once because the chance of message loss is low. If a failure occurs, you might not retry, or you can rely on higher-level recovery. At-most-once avoids duplicates. Option 1 is at-least-once, which could lead to duplicates if retries happen. Option 2 is hard to achieve. Option 3 is not standard."
  },
  {
    "id": 168,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "What problem does a 'circuit breaker' solve in distributed systems?",
    "options": [
      "It prevents electrical failures in data centers.",
      "It stops a client from making requests to a service that is likely to fail, allowing it to recover and preventing cascading failures.",
      "It breaks the connection between services to enforce security.",
      "It limits the rate of requests to a service."
    ],
    "answer": 1,
    "explanation": "A circuit breaker trips when failures exceed a threshold, blocking further requests and giving the failing service time to recover. This prevents cascading failures and wasted resources. Option 2 is about security. Option 3 is rate limiting."
  },
  {
    "id": 169,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the context of logical clocks, what is a 'happened-before' relation?",
    "options": [
      "A relation based on physical time ordering.",
      "A relation defined by local execution order and message passing, forming a partial order of events.",
      "A total order of all events in the system.",
      "A relation that only applies to events in the same process."
    ],
    "answer": 1,
    "explanation": "The happened-before relation (→) is a partial order defined by: (a) same process order, (b) send-receive pairs, and (c) transitivity. It captures causality. Option 0 is physical time. Option 2 is total order, which is stronger. Option 3 is incomplete."
  },
  {
    "id": 170,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "If event A happened-before event B, what can we say about their Lamport timestamps L(A) and L(B)?",
    "options": ["L(A) < L(B)", "L(A) ≤ L(B)", "L(A) > L(B)", "L(A) = L(B)"],
    "answer": 0,
    "explanation": "Lamport clocks guarantee that if A → B, then L(A) < L(B). They preserve causality. Option 1 is not strict enough; they could be equal if not causal. Option 2 and 3 are opposite."
  },
  {
    "id": 171,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In a primary-backup system with asynchronous replication, a client writes to the primary, which acknowledges immediately. The primary then crashes before replicating to backups. What happens to that write?",
    "options": [
      "The write is lost because it was not replicated.",
      "The write is safe because it was acknowledged.",
      "The write may be recovered from the primary's disk if it restarts.",
      "The write is automatically replicated by the backups."
    ],
    "answer": 0,
    "explanation": "With asynchronous replication, the write is acknowledged as soon as the primary processes it, before it is sent to backups. If the primary crashes before replication, the write is lost unless it was persisted on the primary and the primary restarts. But if the primary is permanently down, the write is gone. Option 1 is false; acknowledgment doesn't guarantee durability. Option 2 is possible if primary recovers, but if it's permanently failed, it's lost. Option 3 is false."
  },
  {
    "id": 172,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "Which consistency model guarantees that once a write is completed, all subsequent reads (from any client) will return that value?",
    "options": [
      "Eventual consistency",
      "Causal consistency",
      "Linearizability",
      "Sequential consistency"
    ],
    "answer": 2,
    "explanation": "Linearizability provides the strongest guarantee: once a write completes, all future reads (in real-time order) must see that write. Sequential consistency does not have real-time guarantees; it only requires a total order consistent with program order. Option 0 and 1 are weaker."
  },
  {
    "id": 173,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "A system uses checkpointing with copy-on-write. What is the advantage of this technique?",
    "options": [
      "It allows checkpoints to be taken without stopping the application for long periods, by capturing a consistent snapshot quickly.",
      "It reduces the size of checkpoints by compressing them.",
      "It eliminates the need for stable storage.",
      "It guarantees that checkpoints are always consistent."
    ],
    "answer": 0,
    "explanation": "Copy-on-write allows a snapshot to be taken by quickly creating a copy of the memory pages, then continuing execution. Actual copying happens only when pages are modified. This minimizes downtime. Option 1 is about compression. Option 2 is false. Option 3 is false; it doesn't guarantee consistency by itself."
  },
  {
    "id": 174,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "What is a 'zombie process' in the context of failure detection and group membership?",
    "options": [
      "A process that has crashed but whose state still exists.",
      "A process that is believed to be failed by the group but is still running and may cause inconsistencies.",
      "A process that is waiting for resources.",
      "A process that has been restarted after a crash."
    ],
    "answer": 1,
    "explanation": "A zombie process is one that has been partitioned away or falsely suspected, but continues to operate, potentially causing conflicts when it rejoins. Option 0 is a crashed process. Option 2 is a blocked process. Option 3 is a recovered process."
  },
  {
    "id": 175,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Paxos algorithm, what is the purpose of phase 1 (prepare phase)?",
    "options": [
      "To get a majority of acceptors to promise not to accept lower-numbered proposals and to learn any already accepted value.",
      "To send the actual value to be agreed upon.",
      "To elect a leader.",
      "To commit the value to the learners."
    ],
    "answer": 0,
    "explanation": "Phase 1 ensures that a proposer learns of any already chosen value (from acceptors) and gets a promise that no lower-numbered proposal will be accepted. This is crucial for safety. Option 1 is phase 2. Option 2 is not the primary purpose. Option 3 is phase 2 and learning."
  },
  {
    "id": 176,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, what happens if a candidate receives an AppendEntries RPC from another server claiming to be leader with a term greater than or equal to the candidate's current term?",
    "options": [
      "The candidate ignores it and continues its election.",
      "The candidate steps down and becomes a follower, recognizing the new leader.",
      "The candidate challenges the leader by starting a new election.",
      "The candidate requests a vote from the leader."
    ],
    "answer": 1,
    "explanation": "If a candidate sees a higher term, it steps down and becomes a follower. If the term is equal but the sender is leader, it also steps down because there can only be one leader per term. Option 0 is wrong. Option 2 would cause issues. Option 3 is not defined."
  },
  {
    "id": 177,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using timestamp ordering, each transaction is assigned a unique timestamp. If transaction T1 (timestamp 100) wants to read an item that was last written by transaction T2 (timestamp 200), what happens?",
    "options": [
      "T1 is allowed to read because it is older, and the write is newer.",
      "T1 is aborted because it is trying to read a value from the future (T2 > T1), violating timestamp order.",
      "T1 waits for T2 to commit.",
      "T1 reads the value and updates its timestamp."
    ],
    "answer": 1,
    "explanation": "In basic timestamp ordering, a read must be from a write with a timestamp ≤ the reader's timestamp. If the last write is from a younger transaction (200 > 100), the read is invalid, and T1 is aborted. This ensures serializability. Option 0 is opposite. Option 2 is not typical in basic TO; it may wait in some variants. Option 3 is not correct."
  },
  {
    "id": 178,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "What is a key advantage of the Saga pattern over two-phase commit (2PC) for microservices?",
    "options": [
      "Sagas provide stronger consistency guarantees.",
      "Sagas avoid holding locks for long periods, improving concurrency and scalability.",
      "Sagas are simpler to implement and require less code.",
      "Sagas guarantee exactly-once execution of each step."
    ],
    "answer": 1,
    "explanation": "Sagas release locks after each local transaction commits, so they don't hold locks across the entire workflow. This improves concurrency. Option 0 is false; 2PC provides stronger consistency. Option 2 is debatable; Sagas add complexity with compensating transactions. Option 3 is false."
  },
  {
    "id": 179,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A geo-distributed database uses active-active replication with conflict resolution based on last-write-wins (LWW) using timestamps. What is a potential problem with this approach?",
    "options": [
      "It requires synchronized clocks, and if clocks are skewed, updates may be lost or incorrectly ordered.",
      "It cannot handle concurrent updates; it always picks one, potentially losing data.",
      "It requires vector clocks, which are expensive.",
      "It only works for numerical data."
    ],
    "answer": 0,
    "explanation": "LWW relies on timestamps, which depend on clock synchronization. If clocks are skewed, a later write might have an older timestamp and be incorrectly overwritten. Option 1 is also true but is a feature of LWW; it's designed to pick one. Option 2 is false. Option 3 is false."
  },
  {
    "id": 180,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "Two events have vector timestamps: E1 = [2,1,1], E2 = [1,2,2]. What is the relationship?",
    "options": [
      "E1 happened before E2",
      "E2 happened before E1",
      "E1 and E2 are concurrent",
      "Insufficient information"
    ],
    "answer": 2,
    "explanation": "Compare E1 <= E2: 2 <= 1 false. Compare E2 <= E1: 1 <= 2 true, 2 <= 1 false. So concurrent."
  },
  {
    "id": 181,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is a 'shuffle' and why is it expensive?",
    "options": [
      "It is a transformation that reorders data within a partition; it's cheap.",
      "It is the process of redistributing data across partitions, often requiring network transfer and disk I/O, making it a costly operation.",
      "It is a type of join that is always optimized.",
      "It is the same as a broadcast variable."
    ],
    "answer": 1,
    "explanation": "A shuffle involves moving data across the network to repartition it (e.g., for groupByKey, join). It can involve disk I/O and serialization, making it a major performance bottleneck. Option 0 is wrong. Option 2 is not always true. Option 3 is different."
  },
  {
    "id": 182,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "You have a Spark job that performs a join between a large RDD and a small RDD. Which optimization should you enable?",
    "options": [
      "Broadcast the small RDD to all nodes to avoid shuffling the large RDD.",
      "Use a hash partitioner on both RDDs.",
      "Increase the number of partitions.",
      "Use reduceByKey instead of groupByKey."
    ],
    "answer": 0,
    "explanation": "Broadcast join (also known as map-side join) sends the small RDD to all executors, allowing the large RDD to be joined without shuffling. This is efficient when one dataset is small. Option 1 might help but still requires shuffle. Option 2 is not directly related. Option 3 is for aggregations."
  },
  {
    "id": 183,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "The lecture discusses the 'fallacy that the network is reliable'. Which of the following is a consequence of this fallacy in system design?",
    "options": [
      "Engineers may not implement retry logic, assuming messages always arrive.",
      "Engineers may assume that bandwidth is unlimited.",
      "Engineers may assume that latency is zero.",
      "Engineers may assume that the network is secure."
    ],
    "answer": 0,
    "explanation": "Assuming the network is reliable means you might not handle message loss, leading to failures when packets are dropped. Option 1 is bandwidth fallacy. Option 2 is latency fallacy. Option 3 is security fallacy."
  },
  {
    "id": 184,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "In the context of RPC, what is 'parameter marshalling'?",
    "options": [
      "The process of converting parameters into a byte stream suitable for transmission over a network.",
      "The process of allocating memory for parameters on the server.",
      "The process of validating parameter types.",
      "The process of encrypting parameters for security."
    ],
    "answer": 0,
    "explanation": "Marshalling (or serialization) is the conversion of data structures into a format that can be sent over the network. Option 1 is about memory. Option 2 is type checking. Option 3 is encryption."
  },
  {
    "id": 185,
    "lecture": "Lecture 2 - Communication: RPC & Messaging",
    "question": "A message queue system guarantees at-least-once delivery. A consumer processes a message and then crashes before acknowledging. What will happen?",
    "options": [
      "The message will be lost.",
      "The message will be redelivered to another consumer (or the same one after restart), leading to potential duplicate processing.",
      "The queue will block until the consumer recovers.",
      "The message will be moved to a dead letter queue."
    ],
    "answer": 1,
    "explanation": "Since the message wasn't acknowledged, the queue assumes it wasn't processed and will redeliver it. This is the definition of at-least-once. Option 0 is at-most-once. Option 2 is not typical; queues usually redeliver. Option 3 happens after multiple failures."
  },
  {
    "id": 186,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "Given vector clocks, if VC(A) = [1,2,0] and VC(B) = [1,2,1], which is true?",
    "options": [
      "A happened before B",
      "B happened before A",
      "A and B are concurrent",
      "A and B are the same event"
    ],
    "answer": 0,
    "explanation": "Compare A <= B: 1 <= 1 true, 2 <= 2 true, 0 <= 1 true. So A ≤ B and there is at least one position where A < B (third component 0 < 1). Therefore A → B."
  },
  {
    "id": 187,
    "lecture": "Lecture 3 - Logical Time, Clocks, Snapshots",
    "question": "In the Chandy-Lamport snapshot algorithm, what does it mean if a channel's state is recorded as empty?",
    "options": [
      "No messages were ever sent on that channel.",
      "All messages sent on that channel before the snapshot were received before the snapshot started.",
      "The channel is broken.",
      "The snapshot algorithm failed."
    ],
    "answer": 1,
    "explanation": "An empty channel state means that all messages sent before the snapshot on that channel were received before the receiver recorded its state. In other words, no messages are in transit at the time of the snapshot. Option 0 is possible but not the definition. Option 2 and 3 are incorrect."
  },
  {
    "id": 188,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "In chain replication, if the tail node fails, how is the system reconfigured?",
    "options": [
      "The head becomes the new tail, and the chain reverses.",
      "The previous node in the chain becomes the new tail, and the chain shortens.",
      "A new tail is elected from the remaining nodes.",
      "The system stops processing reads until the tail recovers."
    ],
    "answer": 1,
    "explanation": "If the tail fails, the node before it becomes the new tail. The chain shortens. Reads now go to the new tail. Option 0 is not typical. Option 2 may involve election but the natural order simplifies. Option 3 is not necessary; the new tail serves reads."
  },
  {
    "id": 189,
    "lecture": "Lecture 4 - Replication and Consistency Models",
    "question": "A quorum-based system with N=5 uses R=3, W=3. What is the maximum number of node failures that can be tolerated while still allowing both reads and writes to succeed?",
    "options": ["1", "2", "3", "4"],
    "answer": 1,
    "explanation": "With R=3 and W=3, you need at least 3 nodes alive for both reads and writes. So you can tolerate 2 failures (since 5-2=3). If 3 fail, only 2 remain, insufficient for R=3 or W=3. So max failures = 2. Option 1 is 2."
  },
  {
    "id": 190,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "What is the purpose of a 'heartbeat' in failure detection?",
    "options": [
      "To synchronize clocks between nodes.",
      "To indicate that a process is alive and functioning, allowing others to detect failures if heartbeats stop.",
      "To measure network latency.",
      "To transfer data between nodes."
    ],
    "answer": 1,
    "explanation": "Heartbeats are periodic signals sent to indicate liveness. If a heartbeat is missed, the receiver may suspect a failure. Option 0 is for clock sync. Option 2 is a side effect. Option 3 is for data."
  },
  {
    "id": 191,
    "lecture": "Lecture 5 - Fault Tolerance and Recovery",
    "question": "In the context of rollback recovery, what is 'message logging' used for?",
    "options": [
      "To record all messages sent for debugging.",
      "To enable replay of messages after a crash, allowing a process to reconstruct its state.",
      "To track network performance.",
      "To ensure messages are delivered in order."
    ],
    "answer": 1,
    "explanation": "Message logging records messages so that after a crash, a process can replay them to recreate its state (assuming deterministic execution). This is a recovery technique. Option 0 is a side effect. Option 2 is monitoring. Option 3 is ordering."
  },
  {
    "id": 192,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In the Paxos algorithm, what happens if two proposers simultaneously start phase 1 with different proposal numbers?",
    "options": [
      "They both may get promises from a majority, but then one may fail in phase 2, leading to a liveness issue that requires a leader election.",
      "The algorithm guarantees that one will succeed immediately.",
      "Both proposals will be accepted, leading to inconsistency.",
      "The acceptors will reject both."
    ],
    "answer": 0,
    "explanation": "Concurrent proposers can cause a liveness issue where they keep conflicting and no value is chosen. This is why Multi-Paxos elects a distinguished leader to avoid contention. Option 1 is false. Option 2 is false; only one value can be chosen. Option 3 is not typical; they may accept one."
  },
  {
    "id": 193,
    "lecture": "Lecture 6 - Consensus and Agreement Protocols",
    "question": "In Raft, what is the 'log matching property'?",
    "options": [
      "If two logs contain an entry with the same index and term, then the logs are identical in all preceding entries.",
      "All logs must have the same number of entries.",
      "The leader's log must match the majority of followers' logs.",
      "Entries are committed only if they match on a majority."
    ],
    "answer": 0,
    "explanation": "The log matching property ensures consistency: if two logs have the same index and term for an entry, then all previous entries are the same. This is enforced by the AppendEntries consistency check. Option 1 is false. Option 2 is a consequence. Option 3 is about commitment."
  },
  {
    "id": 194,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "In a distributed database using strict two-phase locking (2PL), what is the 'shrinking phase'?",
    "options": [
      "The phase where a transaction acquires locks.",
      "The phase where a transaction releases locks and cannot acquire new ones.",
      "The phase where a transaction commits.",
      "The phase where locks are upgraded."
    ],
    "answer": 1,
    "explanation": "The shrinking phase is when locks are released. Once a lock is released, the transaction cannot acquire any more locks (to ensure serializability). Option 0 is growing phase. Option 2 is commit, which may happen after shrinking. Option 3 is not a phase."
  },
  {
    "id": 195,
    "lecture": "Lecture 7 - Distributed Transactions & Commit",
    "question": "Consider a Saga for an e-commerce order: Reserve inventory, charge payment, create order. If charging payment fails, what compensating transactions are executed?",
    "options": [
      "Release inventory (compensate for reserve)",
      "Refund payment (compensate for charge) - but charge didn't succeed, so no compensation needed, just abort.",
      "Cancel order (but order wasn't created)",
      "Do nothing; the Saga fails and the user retries."
    ],
    "answer": 1,
    "explanation": "If charging payment fails, the only step that succeeded was reserve inventory. So we need to compensate that step by releasing inventory. The charge step had no effect (or failed), so no compensation for it. Option 1 correctly identifies compensating the reserve. Option 0 is the same. Option 2 is not needed. Option 3 is not a compensating action."
  },
  {
    "id": 196,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "A system uses causal consistency. Which of the following scenarios is NOT possible?",
    "options": [
      "User A posts a message. User B sees the message. User A then edits the message. User C sees the edit before the original message.",
      "User A updates their profile. User B sees the update. User C sees the old profile after seeing the update.",
      "User A and User B concurrently update their statuses. User C sees A's update then B's, while User D sees B's then A's.",
      "User A sends a message to User B. User B replies. User A sees the reply before the original message (due to network delay)."
    ],
    "answer": 3,
    "explanation": "Option 3 violates causality because the reply depends on the original message. Causal consistency requires that causally related operations be seen in order. Option 0 also violates causality (edit depends on original). Option 1 violates monotonic reads if not provided, but causal alone may allow it? Actually, causal consistency alone doesn't guarantee monotonic reads. So option 1 might be possible. Option 2 is allowed for concurrent updates. Option 3 is clearly a causal violation. Option 0 is also a violation. The question asks which is NOT possible, meaning which is forbidden. Both 0 and 3 are forbidden. But if we have to pick one, 3 is a classic example. Option 0 is also a violation. Let's see: edit depends on original, so seeing edit before original violates causality. So both 0 and 3 are violations. The question might expect 3 as the most direct. I'll choose 3."
  },
  {
    "id": 197,
    "lecture": "Lecture 8 - Consistency, CAP & Geo-Distributed Systems",
    "question": "What is the main advantage of using hybrid logical clocks (HLC) in a system like CockroachDB?",
    "options": [
      "They provide a total order of events.",
      "They capture causality while also providing timestamps that are close to physical time, enabling external consistency with real-time ordering.",
      "They eliminate the need for any clock synchronization.",
      "They are smaller than vector clocks but provide the same information."
    ],
    "answer": 1,
    "explanation": "HLCs combine physical time with a logical component. They provide timestamps that are close to physical time (useful for snapshot isolation, etc.) while also capturing causality. This allows CockroachDB to provide external consistency. Option 0 is false; they provide partial order. Option 2 is false; they still need NTP. Option 3 is false; they provide less causal info than vector clocks but are more scalable."
  },
  {
    "id": 198,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "In Spark, what is the difference between `map` and `flatMap`?",
    "options": [
      "`map` returns a new RDD of the same number of elements, while `flatMap` can return zero or more elements per input, flattening the result.",
      "`map` is a transformation, while `flatMap` is an action.",
      "`map` works on key-value pairs, while `flatMap` works on single values.",
      "There is no difference; they are interchangeable."
    ],
    "answer": 0,
    "explanation": "`map` applies a function that returns a single element for each input. `flatMap` applies a function that returns a sequence (optionally empty) and flattens the results. Option 1 is false; both are transformations. Option 2 is false. Option 3 is false."
  },
  {
    "id": 199,
    "lecture": "Lecture 9 - Distributed Data Processing (MapReduce/Spark)",
    "question": "You are tuning a Spark job and notice that the shuffle spill is high. What does this indicate?",
    "options": [
      "The data is being spilled to disk because there isn't enough memory to hold shuffle data, indicating memory pressure.",
      "The network is slow.",
      "The data is skewed.",
      "The serialization format is inefficient."
    ],
    "answer": 0,
    "explanation": "Shuffle spill occurs when the memory allocated for shuffle operations is insufficient, causing data to be written to disk. This hurts performance. Option 1 might cause delays but not spill. Option 2 could cause skew but not directly spill. Option 3 could cause larger data but spill is about memory."
  },
  {
    "id": 200,
    "lecture": "Lecture 1 - Introduction to Distributed Systems",
    "question": "Which of the following is an example of 'migration transparency'?",
    "options": [
      "A user accesses a file using the same pathname even if the file is moved to a different server.",
      "A web server continues to serve requests even after a hardware failure.",
      "A mobile app switches from Wi-Fi to cellular without dropping the connection.",
      "A database replica is added without changing the application's connection string."
    ],
    "answer": 2,
    "explanation": "Migration transparency (or relocation transparency) hides that an object may be moved to another location while in use. The mobile app switching networks is an example of the connection moving without disrupting the user. Option 0 is location transparency. Option 1 is failure transparency. Option 3 is replication transparency."
  }
]
